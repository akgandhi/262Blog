[
  {
    "path": "posts/2021-02-20-test-panel/",
    "title": "Heuristics and Biases",
    "description": "How judgements happen",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-02-20",
    "categories": [],
    "contents": "\nA Machine for Jumping to Conclusions\nSystem 1 is engaged in a constant state of monitoring and assessment.\nThis serves a crucial evolutionary function: humans are confronted with a variety of circumstances in which survival depends on quick/sensible decisions getting made.\nA vestige of this evolutionary past is a strong tendency by System 1 to jump to conclusions based on associative coherence, and suppress ambiguity.\n\nJumping to conclusions is efficient if the conclusions are likely to be correct and the costs of an occasional mistake acceptable, and if the jump saves much time and effort\n\n\nIn each case we are unconscious to the inherent ambiguity.\nSystem 1 suppression of ambiguity leads to the “Halo Effect”\nIf you like one thing about a person you have tendency to like everything (and vice versa)\n\nWhat do you think of Alan & Ben?\n Alan : intelligent, industrious, impulsive, critical, stubborn, envious.  Ben : envious, stubborn, critical, impulsive, industrious, intelligent.\n\n\nThe sequence in which we observe characteristics of a person is often determined by chance. Sequence matters, however, because the halo effect increases the weight of first impressions, sometimes to the point that subsequent information is mostly wasted” —TFS, p. 83\n\nWhat you see is all there is (WYSIATI)\n\nAn essential design feature of the associative machine is that it represents only activated ideas. Information that is not retrieved (even unconsciously) from memory might as well not exist\"\n\nSystem 1 constructs the best possible story incorporating the ideas that have been activated but does not (cannot) allow for info it doesn’t have.\n\nSo what is the problem? Isn’t System 1 (warts and all) is making life functional?\n\n\n\nSystem 1 bets on an answer, and the bets are guided by experience. The rules of the betting are intelligent…WYSIATI facilitates the achievement of coherence and of the cognitive ease that causes us to accept a statement as true. It explains why we can think fast, and how we are able to make sense of partial information in a complex world. Much of the time, the coherent story we put together is close enough to reality to support reasonable action.\n\nThe Mental Shotgun\nWhere things go wrong for our intuitive system is when the mental shotgun takes place.\n]\n\n\nWhat is the “mental shotgun”?\n\nDirecting the mind to a problem is unleashing a mental shotgun: a spray of bullets rather than a targeted hit.\nHow does it work?\nIntention to perform one computation (targeted to System 2) evokes another (hijacked by System 1)\nSystem 1 is faster and so it suggests a solution quickly, and System 2 is lazy.\nExample\n\nParticipants in one experiment listened to pairs of words, with the instruction to press a key as quickly as possible whenever they detected that the words rhymed. The words rhyme in both these pairs:\n\nVOTE—NOTE\nVOTE—GOAT \nThe key to what makes this experiment illuminating are the instructions - to press a key when the words are detected to rhyme.\nWhat actually happened in the experiment? Why?\nSubstituting Questions\n\nI propose a simple account of how we generate intuitive opinions on complex matters. If a satisfactory answer to a hard question is not found quickly, System 1 will find a related question that is easier and will answer it. I call the operation of answering one question in place of another substitution … The idea of substitution came up early in my work with Amos, and it was the core of what became the heuristics and biases approach.\n\nSubstitution can be an effective way to solve problems. It is an example of a heuristic\n\nThe technical definition of a heuristic is a simple procedure that helps find adequate, though often imperfect, answers to difficult questions. The word comes from the same root as eureka\n\nSubstitution was discussed in the George Polya classic book on mathematical reasoning *How to solve it, where he famously wrote “If you can’t solve a problem, then there is an easier problem you can solve: find it.”\nAs Kahneman writes:\n\nPólya’s heuristics are strategic procedures that are deliberately implemented by System 2. But the heuristics that I discuss in this chapter are not chosen; they are a consequence of the mental shotgun, the imprecise control we have over targeting our responses to questions.\n\nThus heuristic is a reasoning strategy that is used to solve a problem , make a decision or form a judgment about something. Original meaning: A heuristic is a strategy that leads to insight, discovery and learning.\nExample: To decide whether Option A is better than Option B.\nE.g., Decide whether to buy a new smart phone with special features.\nHeuristic #1: Assume that various specific situations prevail – Is A better than B in these situations ?\nHeuristic #2: Ask people who chose A how they feel about their choice? Do the same for Option B.\nAttribute Substitution\nPerson wants to evaluate a case with respect to a target attribute.\n Example  You are interviewing job applicants.\nTarget Attribute: How they will perform on the job over the long run?\nThe target attribute is hard to evaluate directly but information about a related heuristic attribute comes readily to mind.\nHeuristic attribute : success of job interview.\nAttribute Substitution: Judgment of target attribute is based on the heuristic attribute.\n Example : Professor hears the talk of a job candidate.\nTarget attribute: How successful will this candidate be in the long run?\nHeuristic attribute: How impressive was the talk?\nHow do these two questions differ?\nObserve that the heuristic attribute is not logically equivalent to the target attribute – it isn’t even highly correlated with it, but it is a salient piece of information that is available to system 1.\nThe Todorov example of impressions of faces of politician’s and electoral outcomes? How was substitution at play there?\nConsider a few more examples.\n\n\nIntensity Matching\n\nSomething is still missing from the story: the answers need to be fitted to the original question.\n\nWhen is Attribute Substitution Likely to Occur?\nAttribute substitution is more likely when:\nthe target attribute is relatively inaccessible, i.e., hard to evaluate or unfamiliar;\na semantically and associatively related attribute is highly accessible (heuristic attribute);\nthe substitution of the heuristic attribute in the judgment is not rejected by critical operations of System 2.\nThe Anatomy of Bias\nImagine a target question that is intended to engage System 2\nThrough the mental shotgun System 1 is engaged.\nSystem 1 seeks associative coherence and cognitive ease\nIt will solve a substitute question instead – something the associative machine is capable of solving.\nReasoning about the substitute questions differs in systematic ways from reasoning about the target\nThe suggestions which are fed to System 2 become hard to overturn/disbelieve – System 2 is lazy\nThey create cognitive illusions – what does it take to disbelieve a visual illusion?\n\nThe 3-D Heuristic\nConsider the image\n\nConsider the question: As printed on the page, is the figure on the right larger than the figure on the left.\nWhat is the substitution that takes place?\n\nThe essential step in the heuristic—the substitution of three-dimensional for two-dimensional size—occurred automatically. The picture contains cues that suggest a 3-D interpretation. These cues are irrelevant to the task at hand—the judgment of size of the figure on the page—and you should have ignored them, but you could not. The bias associated with the heuristic is that objects that appear to be more distant also appear to be larger on the page. As this example illustrates, a judgment that is based on substitution will inevitably be biased in predictable ways. In this case, it happens so deep in the perceptual system that you simply cannot help it.\n\nThe Mood Heuristic\nConsider the questions\nHow happy are you these days? How many dates have you had in the last month?\nKey finding - zero correlation. Apparently dating did not systematically come to mind in consideration of overall happiness.\nHowever consider the flip\nHow many dates have you had in the last month? How happy are you these days?\nWhat happened? Why?\nThe key idea is that “Happiness these days” is not an easy or natural assessment - the mental shotgun kicks into action.\nThe role of “unbelieving” is left to System 2\nWe are rarely stumped\nHave intuitive feelings & opinions about everything\nLike or dislike on sight\nTrust or distrust on sight\nThe ability to question/doubt/probe/debunk is tied to System 2.\nGilbert “Bias to Believe” experiments (pages 80-81)\n\nLazy or Busy/Depleted System 2 endorses our intuitive beliefs\nSystem 2 acts as biased lawyer\nEven when it tries to probe, it is still hamstrung.\nSearches for information consistent with existing beliefs\nActs as apologist not critic\nFights in the court of public opinion to persuade others of system 1 ’s view\n\nConfirmation Bias\n\n\nAnchoring Heuristic\n\nIs the average yearly rainfall in Miami ( Florida) greater or less than ….\nNow write down your best guess as to the average yearly rainfall in Miami .\n\nCondition 1: 2 inches/year\nCondition 2: 750 inches/year\nWhat do you think is the typical finding? Why is this associated to the anchoring heuristic?\nFYI : Average yearly rainfall in Miami is 61.9 inches per year (157.2 cm/year). Average yearly rainfall in Seattle is 37.7 inches (95.8 cm).\nWhat is the anchor and adjust heuristic:\nStep 1: Consider an initial estimate of the quantity you are trying to judge. This is the “anchor.” (People often know that this initial estimate isn’t perfectly accurate.)\nStep 2: Adjust the initial estimate in the direction that corrects for assumed sources of error.\nPsychological Fact: Adjustments are typically too small!\nResult : Final judgment is overly influenced by the anchor, i.e., the final estimate is biased towards the anchor.\nWhile this might seem like a reasonable estimation strategy, it has perverse effects that are evocative of a System 1 bias. Consider the wheel of fortune example from TFS.\n\n\nBuilt to stop at 10 & 65 (randomized between groups). Write down the number that the wheel stops.\n\nNow answer the question:\nIs the percentage of African nations among the UN members larger or smaller than the number you just wrote?\nWhat is your best guess of the percentage of African nations in the UN?\nThose who wrote down (10) guessed on average 25%, and those who wrote down 65 guessed on average 45%.\nConsider another example\nAnnual donation “to save 50000 offshore Pacific Coast seabirds from small offshore oil spills until ways are found or prevent spills or require tanker owners to pay for the operation.”\nNo anchor ($65)\nWould you be willing to donate 5 dollars? (20 dollars)\nWould you be willing to donate 400 dollars? (143 dollars)\nWhy Are Estimates Biased Towards the Anchor?\nAdjustments stop when you reach the range of values which seem to be possible answers.\nE.g., I think that the rainfall in Miami must be between 20 - 80 inches per year. If the anchor = 2 inches, I adjust until my estimate is over 20. If the anchor is 600 inches, I adjust until my estimate is under 80.\nCognitive Laziness: I am too quick to terminate the process of finding the “best” estimate.\nPriming: The anchor primes similar thoughts which influence the production of the estimate.\n\nThink about scenarios in life where anchoring applies (e.g.,any bargaining situation)\nAssume any number has an anchoring effect and mobilize system 2 to combat it.\nExample: Suppose a biased or unreliable news source tells you that something extreme will happen, e.g , next year 50% of retail banks will fail.\nYou don’t trust this news source, so you adjust the estimate from 50% to something you think is more realistic, but your adjustment will typically be too small.\nExample: Bargaining with seller.\nSuppose you want to buy a useful but ordinary rug. Under usual circumstances, you would be willing to pay $100 for the rug. Suppose the seller says that he will sell the rug for $500. What is your counter offer?\nFalse Consensus Efect\nExample: Suppose you are asked whether many people believe X or many people feel X.\nE.g., What proportion of Americans favor repeal of Obama care (Affordable Care Act)?\nHypothesis: People often anchor on their own opinions and values and then adjust to take into account other people’s differences. Consequence: We tend to expect others to be more like ourselves than they are.\n\n\n\n",
    "preview": "https://holisticelephants.files.wordpress.com/2018/06/cog-bias-5.png?w=960&h=593&crop=1",
    "last_modified": "2021-02-25T14:10:48+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-18-user-persona/",
    "title": "Defining a User",
    "description": "Who will benefit from your solution and how will they experience value?",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-02-18",
    "categories": [
      "Semester Project",
      "Human-Centered Design"
    ],
    "contents": "\nNow that you have a problem you are trying to impact and a first iteration design question, lets start to converge on a specific scenario where your efforts will be impactful.\nThe ultimate goal will be to collect evidence for the hypothesized elements in your design thinking. Evidence will take a structured form. Here is an example used by AI teams at Google.\n\nYou will collect this information by talking to people - e.g., User Interviews.\nBut who should you ask?\nThis amounts to understanding your audience better, which in turns gives you a deeper appreciation for your problem and design question.\nClass Activity\n\nUser Persona\nUse the define your audience exercise to build 3 User Personas - e.g., personality profiles of your potential users. Your goal is to find users and interview them and have one user from each prototype.\nThere are many online tools available to help you develop flashy user personas. Feel free to adopt any such took to guide your thinking.\nHowever for the purpose of our second milestone, we will use the simple medium of google docs. Adopt the Pet Resume template in Google docs and build 3 User Personas.\n\nPart of the goal of defining the user (and ultimately improving their lives in some fashion) is to narrow their pain points so you know what to rapidly prototype. Consider the advice from a human-centered designer:\n\nI like to use the problem youth sports sign-ups in my courses and workshops. Being a mom of four kids, I really struggle to sign them up for sports in a hassle-free, efficient way. Something always seems to go wrong. When designing for a persona whose frustration is dealing with complicated scheduling, the wireframe should reflect a solution to frustration by displaying the time and date of the practices and games prominently. They shouldn’t just be another line of text. They should be a heading or in their own column. It’s important to have them stand out so that the user will have their frustration alleviated or at least reduced due to clear communication on the scheduling of the sports events. This is just one way to accommodate this frustration, but it’s straightforward giving us a clear sense of how a person directly ties into a wireframe.\n\nHere are the dimensions to reflect in your profiles:\nName\nDemographic data\nPicture\nBackground\nBehaviors/Hobbies/where they spend time making decisions\nProblem/Pain/Frustrations\nUrgency/Intensity\nAnswer to the question: What is most important is to connect the emotions of your persona with the problem being addressed?\n\n\n\n",
    "preview": "https://99designs-blog.imgix.net/blog/wp-content/uploads/2018/01/Marketing-personas.jpg?auto=format&q=60&w=1860&h=1860&fit=crop&crop=faces",
    "last_modified": "2021-02-18T18:16:46+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-16-associative-machine/",
    "title": "Associative Machine",
    "description": "A closer look at the thinking patterns of System 1",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-02-16",
    "categories": [],
    "contents": "\nThe “Associative Machine”\nA major feature of System 1 is the ability/speed to function as an “associative machine”, e.g., form links between ideas/actions/events in associative memory and develop causal and coherent accounts of these linkages.\nThus, for example if you see a dog, you expect to see his master. Alternatively, if you hear a breaking sound, you expect to see that something has been broken.\nCertain effortless instincts and associations of System 1 are learned automatically from birth, e.g., detect hostility in a voice.\nOther automatic activities of System 1 become learned through prolonged practice that creates associations between ideas, e.g., the expertise of a chess master in reading a board.\nFrom this network of associations stored in memory, System 1 can create inference/understanding/accounts of sensory data in the environment in automatic and instantaneous fashion.\nAs TFS describes:\n\nthe knowledge is stored in memory and accessed without intention and without effort.\n\nAs an example of the associative phenomena, consider the following example of two words on a page:\n\n Banana Vomit \n\n\nOne of the first things System 1 experience upon reception of these words is a mild element of surprise – it is uncommon to see these words juxtaposed in this fashion (perhaps having never seen together at this point).\nNevertheless, the associative machine will immediately work to tie these two together using properties of associative memory.\nThe most likely association it will create is a temporal sequence whereby the banana caused the sickness inducing the vomit. This gravitation towards associative coherence will take place automatically without our conscious intervention or effort.\nMoreover, once this association is now formed and placed (at least temporarily) in memory, it will automatically activate a cascading network of related associations that can affect our behavior- a phenomena known as priming.\nExperience unpleasant images and memories\nface twists slightly in disgust\nheart rate increases\nhair on arms rises a little\nsweat glands activated\nattentuated version of how you would react to actual event, beyond your control\ntemporary aversion to bananas\nIn this case, we will feel a momentary physical aversion to bananas, and we will be more attuned receptive to concepts related to “vomit” and “bananas” – sick, stink, nausea, yellow, fruit, apple, berries.\nThe mechanisms of association of ideas should be seen as nodes in a large network (associative memory) in which each idea is linked to many others.\nThe linkages can take have different underlying meaning/interpretation such as\nCauses linked to effects (virus -> cold)\nThings linked to their properties (lime -> green)\nThings to the category in which they belong (banana -> fruit)\nThe association of ideas was understood by Enlightenment philosophers John Locke and David Hume as a basis for human intelligence.\nThe modern scientific appreciation to the subject adds to this philosophical tradition by recognizing that each when a node in this network is activated in memory, it triggers activation of the entire linked network of ideas/concepts all at once in a manner that is automatic (System 1) and hidden from our conscious selves.\nThe activation mechanism is symmetric - causes activate effects and effects activate causes.\nHebbian Learning\nD. O Hebb\n“When an axon of cell A is near enough to excite a cell B and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that A’s efficiency, as one of the cells firing B, is increased.”\n— D. O. Hebb (1949)\n\nD. O HebbOr said another way….\n\n“Neurons that fire together, wire together. Neurons that fire out of sync, fail to link.”\n\nneuronDendrites : Act as the channel that gets signals to cell body with their strength as weights\nCell Body : Collects input through dendrites and processes to produce output\nAxon : Responsible for transmitting signals to other neurons\nA biological neural network consists of a collection of neurons and their synaptic connections.\nbiological neural netAt a point in time the network is characterized by a state - a pattern of activity (on/off) of the individual neurons.\nNeurons update their activity based on the input the receive over the synapses. Information is processed by the network through activity spread.\nSynaptic Strength\nsynaptic strengthSynapses are the links between neurons or between neurons and fibers carrying external input.\nA synapse carries pre-synaptic activity to the post-synaptic site.\nSynapses have differential strength, reflecting the amount of energy it transmits to the post-synaptic neuron from activity at the pre-synaptic site.\nPriming\nThe functioning of this associative machinery leads to a certain “manipulability” of System 1 through associative cues known as priming.\nPriming arises when exposure to one idea makes an individual more receptive to the suggestion of another idea due to familiarity induced by change in the state of memory from the network of underlying associations.\nA mundane illustration of priming is when exposed to “eat” it makes it more likely someone fills in the blank of\n\n SO_P \n\n\nwith  SOUP  rather than  SOAP  (and vice versa if exposed to “wash” rather than “eat”).\nHowever, there are some more non-trivial manifestations of priming that Chapter 4 documents.\nThe Florida Effect\nAmong the most famous of these is the “Florida effect”.\nJohn Bargh at NYU conducted a study with students (aged 18 – 22) who were asked to assemble 4-word sentences from 5 words Eg: “finds he it yellow instantly”. The treatment group was asked to unscramble sentences that included words such as\n\nFlorida, forgetful, bald, grey, wrinkle\n\n\nThe students were then asked to walk down the hall to complete another task, but it turned ou this walking trip was the heart of the experiment.\nThe key finding is that the treatment group walked down significantly slower than others!\nThe “Florida effect” involves two stages of priming. First the words exposed to the treatment effects primes ideas of old age (observe the word “old” is never mentioned). Second, old age primes a behavior, namely walking slowly.\nThe “Bargh lab” has produced some other notable priming effects. Consider the example in the video.\n\n\n\n\n\n\n\nAn important dimension of the hypothesized effect is that the priming mechanism also works through the reciprocal links of the associative machine.\nA study conducted at a German university showed the Florida effect in reverse. Students were asked to walk around a room for 5 minutes at 1/3 of the normal pace (30 steps per second). Afterwards they were much quicker to recognize words associated with old age.\nLikewise, in another study, tests were done while subjects are smiling/frowning (induced by a pencil held in the mouth) or nodding/shaking head. Depending on the treatment, these groups differed in finding cartoons funnier, upsetting pics worse, and radio editorials more or less agreeable.\nPriming effects play a major role in how firms attempt to market products to consumers. More recently, priming effects have launched theories and practices as to how to induce specific influences and behaviors in others through\n\n\n\n\n\n\n\nEndorsing Intuitions: Cognitive Ease\nWhen System 1 can readily achieve cognitive coherence, it provides a sensation of cognitive ease. It is lack of cognitive ease (or cognitive strain) that is one of the warning signs that mobilizes System 2. However, when cognitive ease is present, the endorsements of System 1 are more readily accepted by System 2.\nThe feeling of cognitive ease is embedded in its own associative network of ideas internalized by System 1, and described on P.60.\n\nWhen an idea/concept/communication has been repeated in the past, is displayed/communicated clearly, has been primed in advance, and conveyed when the subject is in a good mood, these all enhance the chances for the experience of cognitive ease.\nLikewise when the subject experiences cognitive ease associated with a stimuli, the subject is more likely to experience affects like the feeling of truth, feeling of familiarities, and feeling good/effortless.\nThis diagram has a variety of behavioral implications. It suggests that when cognitive ease is achieved, then statements are more likely to be believed.\nGiven that mood, familiarity, clarity, priming affect cognitive ease, then one implication is that if you want your message to be believed by a person, you should use repetition, simple and clear language, have positive/likeable demeanor to affect mood, and prime (think “pre-suasion”) for your message.\nThis also implies the opposite result – when pre-conditions for cognitive ease are not realized (and cognitive strain is experienced) then System 2 is more likely to scrutinize the plausibility of conclusions. Consider the questions designed by Shane Frederick’s “Cognitive Reflection Test”\n\n A bat and a ball cost $1.10  The bat costs one dollar more than the ball  How much does the bat cost?  Or  All roses are flowers  Some flowers fade quickly  Therefore some roses fade quickly \n\n\nIn each case, the intuitive answer (1 dollar and yes) is the wrong answer that you can check with half a minute of deliberative thought. In fact, More than 50% of Harvard, MIT and Princeton students give the wrong answer – at other universities this is as high as 80%.\nHowever as Kahneman argues\n\nDoes this mean we are subject to inferior thinking due to our generally lazy System 2 behavior which is only incited due to a signal of strain from System 1. To the contrary, the ability of System 1 to sift through ideas in search of patterns of association that create cognitive ease is one of the drivers of creativity. See the findings on good feeling and the Remote Association Test on pages 68-69.\n\nThis suggests two distinct clusters of cognitive experiences.\n\nA Machine for Jumping to Conclusions\nSystem 1 is engaged in a constant state of monitoring and assessment.\nThis serves a crucial evolutionary function: humans are confronted with a variety of circumstances in which survival depends on quick/sensible decisions getting made.\nA vestige of this evolutionary past is a strong tendency by System 1 to jump to conclusions based on associative coherence, and suppress ambiguity.\n\nJumping to conclusions is efficient if the conclusions are likely to be correct and the costs of an occasional mistake acceptable, and if the jump saves much time and effort\n\n\nIn each case we are unconscious to the inherent ambiguity\nHalo Effect\nThe System 1 suppression of ambiguity leads to the “Halo Effect”\nIf you like one thing about a person you have tendency to like everything and vice versa.\nWhat do you think of Alan & Ben?\n\nAlan: intelligent, industrious, impulsive, critical, stubborn, envious. Ben: envious, stubborn, critical, impulsive, industrious, intelligent.\n\n\nAs Kahneman explains:\n\nThe sequence in which we observe characteristics of a person is often determined by chance. Sequence matters, however, because the halo effect increases the weight of first impressions, sometimes to the point that subsequent information is mostly wasted” \\(TFS\\, p\\. 83\\)\n\nImplications for polarization of attitudes and beleifs among different groups?\nWhat you see is all there is (WYSIATI)\n“An essential design feature of the associative machine is that it represents only activated ideas. Information that is not retrieved even unconsciously from memory might as well not exist”\nSystem 1 constructs the best possible story incorporating the ideas that have been activated but does not cannot allow for info it doesn’t have.\n“Will Mindik be a good leader? She is intelligent and strong…”\nSystem 1 has a coherent association and an answer…yes!\nImportantly, what did System 1 not do:\n\n“What would I need to know before I formed an opinion about the quality of someone’s leadership”?\n\n So what is the problem? \nIsn’t System 1 warts and all making life functional?\n\nSystem 1 bets on an answer, and the bets are guided by experience. The rules of the betting are intelligent…WYSIATI facilitates the achievement of coherence and of the cognitive ease that causes us to accept a statement as true. It explains why we can think fast, and how we are able to make sense of partial information in a complex world. Much of the time, the coherent story we put together is close enough to reality to support reasonable action.\n\n\n\n\n",
    "preview": "https://upload.wikimedia.org/wikipedia/commons/8/87/Priming_Web_Diagram.svg",
    "last_modified": "2021-02-16T17:37:28+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-10-choice-architecture/",
    "title": "Choice Architecture",
    "description": "Designing Nudges",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-02-10",
    "categories": [],
    "contents": "\n\nNudge\nIN Richard Thaler’s Nudge, he references the fact that we as humans have an automatic system and a reflective system that guide our choices\nWhat Nudge calls the automatic system, Thinking Fast and Slow calls System 1\nWhat Nudge calls thereflective system, Thinking Fast and Slow calls System 2\nThere is no need to worry about the reflective system under this view\nBut the automatic system can make mistakes, especially when confronted with complexity and ambiguity.\nThe central argument of the book is that it is possible to modify the choice environment facing a human to ensure fewer mistakes by the automatic system in every one of us.\nThese modifications are called nudges, and the people designing them in the language of the book are choice architects.\nThe ideas get brought to life in the introductory example of the book.\nThe Caffeteria Example\n\nA friend of yours, Carolyn, is the director of food services for a large city school system. She is in charge of hundreds of schools, and hundreds of thousands of kids eat in her cafeterias every day. Carolyn has formal training in nutrition (a master’s degree from the state university), and she is a creative type who likes to think about things in nontraditional ways.\n\n\nOne evening, over a good bottle of wine, she and her friend Adam, a statistically oriented management consultant who has worked with supermarket chains, hatched an interesting idea. Without changing any menus, they would run some experiments in her schools to determine whether the way the food is displayed and arranged might influence the choices kids make. Carolyn gave the directors of dozens of school cafeterias specific instructions on how to display the food choices. In some schools the desserts were placed first, in others last, in still others in a separate line. The location of various food items was varied from one school to another. In some schools the French fries, but in others the carrot sticks, were at eye level.\n\n\nFrom his experience in designing supermarket floor plans, Adam suspected that the results would be dramatic. He was right. Simply by rearranging the cafeteria, Carolyn was able to increase or decrease the consumption of many food items by as much as 25 percent. Carolyn learned a big lesson: school children, like adults, can be greatly influenced by small changes in the context. The influence can be exercised for better or for worse. For example, Carolyn knows that she can increase consumption of healthy foods and decrease consumption of unhealthy ones.\n\n\nWith hundreds of schools to work with, and a team of graduate student volunteers recruited to collect and analyze the data, Carolyn believes that she now has considerable power to influence what kids eat. Carolyn is pondering what to do with her newfound power. Here are some suggestions she has received from her usually sincere but occasionally mischievous friends and coworkers:\n\nDesign Options\nArrange the food to make the students best off, all things considered.\nChoose the food order at random.\nTry to arrange the food to get the kids to pick the same foods they would choose on their own.\nMaximize the sales of the items from the suppliers that are willing to offer the largest bribes.\nMaximize profits, period.\nChoice Architecture\nCarolyn is what we the book Nudge calls a choice architect. A choice architect has the responsibility for organizing the context in which people make decisions.\nJust like in real architecture, in choice architeccture there is no “neutral design”. In real architecture, whatever architectural design is implemented, it will have implications for how the building is used and hence impacts the overall functionality of the building. Altering the details of the chocie environment so as to influence behavior is choice architecture.\n\nAnd just as a building architect must eventually build some particular building, a choice architect like Carolyn must choose a particular arrangement of the food options at lunch, and by so doing she can influence what people eat. She can nudge.\n\n\nA nudge, as we will use the term, is any aspect of the choice architecture that alters people’s behavior in a predictable way without forbidding any options or significantly changing their economic incentives. To count as a mere nudge, the intervention must be easy and cheap to avoid. Nudges are not mandates. Putting the fruit at eye level counts as a nudge. Banning junk food does not.\n\nThe analogy with physical architecture helps us see market design as borrowing directly from product design. Just as humans use products, humans make decisions. Just as objects should be designed so humans intuitively use them correctly and successfully, so should choice contexts be designed so humans are readily able to make successful decisions.\nOne of the pioneers of human centered design in realtion to objects is Don Norman, and many of the principles he develops can be imported to choice architecture and market design\n\n\n\n\n\n\n\nGood Choice Architecture\nThe book (and paper) offer 6 principles of good choice architecture that are identified through the similarity and connection with product design.\nPrinciple 1: Set Good Defaults\nEvery choice situation has a default choice, whether it is made explicit or not\nThe default is what a chooser gets when she chooses nothing\nWhen people are required to choose one item from a list, it is often helpful if the choice architect specifies a default choice that would be a good choice for most people, especially those who need help in choosing\nIt is evident that many products and objects are designed with good default behavior:\nComputers that switch off when unattended, auto headlights that turn off when the engine is off\nAs most people will keep the factory-set default value of the length of time an unattended computer waits before it switches off, it is important that this default be chosen carefully to help most computer users, especially the least savvy\nSoftware downloads often come with a choice between a “recommended” download and a “custom” download.\nIt is a good idea to help non-expert downloaders by designing the recommended download so that it is a good choice for most non-experts.\nThis recommended option should be pre-selected, not just offered\nHowever, the custom option should be available for those confident enough to use it\n\nA specific tendency of System 1 where choice architecture and nudging has been used with great effect is Status Quo Bias\nStatus quo bias ia a tendency of System 1 and provides a path through which choice architecture can improve decisions by appealing directly to System 1.\nThe following video from a talk by Dan Ariely provides a vivid example of choice architecture in action.\n\n\n\n\n\n\n\nStatus Quo Bias in Policy\nA provision in the No Child Left Behind Act requires that school districts supply the names, addresses, and telephone numbers of students to recruiters in the armed forces\nHowever, the law also says that parents have the right to have their children’s information not given to the recruiters\nThe Fairport, NY school district interpreted the law as an ‘opt-in’ policy: information is given to recruiters only for those students whose parents give explicit permission\nThe Defense Department on the other hand insisted that the law was an ‘opt-out’ policy: information will have to be given to recruiters for all students except the students whose parents explicitly refuse permission\nBoth sides understood the power of the default option\nDefault Choice or Mandated Choice?\nInstead of specifying a default choice—a choice that will be applied to those who make no choice—the choice architect could force every individual to make a choice\nForcing a choice is a good idea when it is important to get the chooser to think hard about the choice and come to a conclusion, instead of relying on the default\nThis is likely to be true especially when\nthere is no default that is likely to be the right choice for a large number of people, and\nmany people will likely end up hating the default\n\nHowever, mandated choice is not helpful when the choice requires expert knowledge.\nIn such cases, most people would be happier accepting the suggestion of an expert\n\nMoreover, the choice may not be a simple yes or no choice. It may require choices about innumerable little details.\nIn such cases, it may be easier to simply rely on the default\n\nPrinciple 2: Expect Error\nA well-designed product or system anticipates the errors that its users are likely to make and helps them avoid such errors\nCard readers that read data no matter how the card is inserted\nCars that warn you if you are not wearing seat belts, are running out of gas, should have the engine checked, etc.\nDifferent nozzles for different fuels at a gas station, so that a car does not get the wrong fuel\nThe hose and the delivery port should be unique to each important injected drug so that chances of confusion are minimized\nThe use of placebo pills to regularize dosage for drug compliance\nMedical checklists for physicians to reduce infection rates (Atul Gawande’s books)\nThe “look right” signs at London crosswalks\n\nPrinciple 3: Give Feedback\nChoice architects should warn users when they are probably making mistakes\nYou are much likelier to take better pictures if you use cameras (digital or polaroid) that give instant feedback, than if you use film cameras that require a lot of time and effort to develop film into photographs (was a relevant issue at the time of the book’s writign!)\nWarnings signs and lights from computers and cars (although recognize the boy who cried wolf risk)\nCeiling paint that is pink when wet and white when dry\nPrinciple 4: Mapping Choices to Welfare\nPeople make better choices when they have help in understanding what the various choices means in terms of their personal happiness\nInstead of telling customers the number of megapixels in a typical photo taken by a camera, tell them what is the largest recommended print size\nFor complex products, especially financial products such as credit cards, insurance, mortgage, etc, one usually has to consider many features of the product and also to anticipate how one would utilize the product and its various features\nHere is a type of libertarian paternalistic \\(nudge\\) called RECAP that Thaler discusses (the technological mechanism is outdated to some degree, but the idea remains useful)\nRecord, Evaluate, and Compare Alternative Prices \\(RECAP\\):\nRequire the seller of any financial product to provide\na downloadable spreadsheet that shows all fees\n\nRequire the seller of a financial product purchased by an individual to provide\na downloadable spreadsheet that shows how the consumer used the product in a typical period in the past\n\nThese documents would enable the consumer to generate, in a spreadsheet, an accurate measurement of the full costs that he/she actually paid for the product he/she is using …\n… and simulations of what the costs would be under alternative patterns of use of the product or similar products\nThis would make it a lot easier to compare alternative complex products\nPrinciple 5: Structure Complex Choices\nWhen people need to choose one item from a long list and evaluate each item by another long list of criteria, people use rules-of-thumb\nFor example, an apartment hunter may restrict his choice to only those apartments with a commute less than 30 minutes\n\nSuch strategies are imperfect; one may miss out on an outstanding apartment with a 32-minute commute\nNaturally, nudges are needed\nConsider how paint color options are presented at a paint store.\nOnline stores have vast collections. But they also offer\nsophisticated search capabilities by various categories, and\nusers’ recommendations and comments\ncorrelation-based recommendations\nSuch structuring of choices make choosing easier\n\nRelationship to the paradox of chocie?\nPrinciple 6: Make Incentives Salient\nWhile human beings respond to nudges, they also respond to incentives\nMake sure users have the right incentives\nMake the incentives salient \\(or prominent\\) so that people don’t miss them\nPeople tend to overweight the costs of a cab ride and underweight the full costs of having a car\nA tax on energy use will have a bigger effect if the thermostat tells the user the savings from turning the temperature down by a degree\n\nNUDGES\ni N centives\nU nderstand how choices correspond to happiness\nD efaults\nG ive feedback\nE xpect error\nS tructure complex choices\n\n\n\n",
    "preview": "https://static.openlawlab.com/uploads/2015/04/ddcae52e65a7efe8e678066d54595830.jpeg",
    "last_modified": "2021-02-11T14:09:21+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-09-system-1-and-system-2/",
    "title": "System 1 and System 2",
    "description": "Introducing the two main characters of the story",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-02-09",
    "categories": [
      "Behavioral Economics"
    ],
    "contents": "\nIntroduction\nIn the 1970s believed that people are generally rational, with strong emotions being the cause departure from rationality. Since then behavioral psychologists and economists have developed a different view based on an analysis of how we think. TFS describes two systems of thought that synthesizes this understanding of the mind’s dual operations: System 1 and System 2. These two Systems play the role of distinct characters in the book, with unique personality attributes who both cooperate yet compete for control of our attention and behavior.\nTo isolate the behavior of System 1, consider the picture\n\nIn fairly effortless manner, we are able to surmise several facts about this woman: she is angry and quite likely to say unkind words in a loud and harsh voice. This assessment came to mind instantaneously without effort, and is an example of fast thinking.\nOn the other hand, consider the question\n17 X 24 =\nWe can immediately identify this as a multiplication problem, and are aware there exists a paper and pencil solution. We can recognize that 12,000 is too big and 123 is too small to be the answer. However, without further thought it is unlikely that we can surmise the correct solution was not 568. In order to carry out the analysis a different mode of thinking needs to ensue – slow thinking. You will retrieve from memory a cognitive program learned in school for carrying out multiplication. You effortfully need to carry out a sequence of steps, retaining numbers in working memory to hold onto intermediate results which feels like a temporary strain. The overall process of mental work is deliberate, effortful, and orderly, a prototype of slow thinking. The effects of slow thinking were not only felt in the mind – it had physical impact on you – your muscles would have tensed, blood pressure rose, heart rate increased, and pupils dilated, which all would have relaxed upon when you found the right answer.\nSystem 1 and System 2\nOther examples of System 1 and System 2 in action include:\nTable 1\nSystem 2\nSystem 1\n\nBrace for the starter gun in a race\n\n\nDetect one object is more distant than another\n\n\nFocus attention on the clowns in a circus\n\n\nOrient the source of a sudden sound\n\n\nLook for a woman with white hair\n\n\nComplete the phrase “bread and …”\n\n\nSearch memory to identify a surprising sound\n\n\nMake a “disgust face” when shown a horrible picture\n\n\nMaintain a faster walking speed than is natural for you\n\n\nDetect hostility in a voice\n\n\nMonitor the appropriateness of your behavior in a social situation\n\n\nAnswer to 2 + 2 =?\n\n\nCount the occurrences of the letter a in a page of text\n\n\nRead words on large billboards\n\n\nTell someone your phone number\n\n\nDrive a car on an empty road\n\n\nPark in a narrow space (for most people)\n\n\nFind a strong move in chess (if you are a chess master)\n\n\nCompare two washing machines for overall value\n\n\nUnderstand simple sentences\n\n\nFill out a tax form\n\n\nRecognize that a “meek and tidy soul with a passion for detail” resembles an occupational stereotype.\n\n\nCheck the validity of a complex logical argument\n\n\nAs TFS describes it\n“System 1 operates automatically and quickly, with little or no effort or sense of voluntary control”\n*“System* 2 allocates attention to the effortful mental activities that demand it, including complex computations. The operations of System 2 are often associated with the subjective experience of agency, choice, and concentration”.\nThe key theme of the book is that whereas our self-identities (and the ideal aspects of Homo Economicus) are bound in the version of our System 2 selves, System 1 in fact drives the lion’s share of our everyday thoughts and actions. A key focus of the book then is to investigate the specific tendencies of System 1 to reach judgments and make choices which are fast, often intelligent, but exhibit systematic biases. The other question the book considers are the capabilities of System 2 that are directed towards control and the correction of the automatic processes of System 1.\nThe key distinguishing characteristics between System 1 and System 2 can be describe more fully as follows.\nSystem 1 (Intuitive)\nSystem 2 (Reflective)\nProcess Characteristics\nAutomatic\nEffortless\nAssociative\nRapid, parallel\nSkilled action\nProcess Characteristics\nControlled\nEffortful\nDeductive (formal rules of reasoning)\nSlow, serial\nRule application\nContent Characteristics\nAffective (emotional)\nCausal propensities (relationships)\nConcrete, specific\nPrototypes\nContent Characteristics\nNeutral\nStatistics\nAbstract\nSets\nHeuristic Reasoning Homo Economicus\nThe Stroop Task\nA useful way to recognize the simultaneous joint existence of these two systems of thinking, consider the “Stroop Task”:\nDirection: State the color in which each word is printed.\nStroop TaskThe “Stroop effect” is that Set 3 is much slower than Set 1 or Set 2 to complete. Why is Set 3 harder than Set 1 or Set 2? We can understand the effect through the joint operations of System 1 and System 2.\nInterpretation of Stroop TaskAn explanation of the Stroop Effect based on the above schematic is as follows. Reading is a highly practiced, automatic skill. System 1 thus accesses the word meaning (concept of red\nor concept of blue) quickly and automatically. This process is involuntary and cannot easily be suppressed or inhibited. The task instruction itself (in the directions) requires following a rule: “Name the font color”. The System 2 rule processing is slower than the automatic reading process of System 1. There is a response conflict between saying “blue” when looking at  red . Also, there is conflict between saying “red” when looking at a  blue . The response conflict in turn causes slower response.\nStroop effect exemplifies two concurrent processes of thinking:\nAutomatic process -> extracting the meaning of “blue”\nControlled process -> identifying the color of “blue”\nReading the word meaning cannot be voluntarily inhibited and a difficulty occurs when an automatic process (extracting the meaning of a color word) conflicts with a rule-governed process (naming the color). A key idea is that the automatic and controlled processes can both occur at the same time, and sometimes they come into conflict. Herein will lie a key source of tension that cause human departures from Homo Economicus: Heuristic reasoning conflicts with rule-based reasoning and can sometimes dominates.\nGorilla Experiment\nAnother phenomenon that reveals is the tension between System 1 and System 2 is this famous experiment by Chabris and Simmons.\n\n\n\n\n\n\n\nThe experiment shows that both System 1 and System 2 compete for the finite attentional resources of the mind. These attentional resources can be associated with the “working memory” module in the schematic model of cognitive resources below (taken from Hastie and Dawes, 2010). For System 2 to perform, it has to assume resources of working memory (through its executive function and control), threby limiting the capacities available for System 1.\n\nInteraction between System 1 and System 2.\nHow are System 1 and System 2 linked? TFS explains the connection rather astutely (p. 24):\n\nIn the story I will tell, System 1 and 2 are both active whenever we are awake,. System 1 runs automatically and System 2 is normally in a comfortable low effort mode, in which only a fraction of its capacity is engaged. System 1 continuously generates suggestions for System 2: impressions, intuitions, intentions, and feelings. If endorsed by System 2, impressions and intuitions turn into beliefs, and implulses turn into voluntary actions. When all goes smoothly, which is most of the time, System 2 adopts the suggestions of System 1 with little or no modification…. When System 1 runs into difficulty, it calls on System 2 to support more detailed and specific processing that may solve the problem of the moment.\n\nExamples of problems that activate System 2’s involvement include all those listed on Table 1, which can range from solving a math problem, exhibiting self-control in specific social situations, heightened awareness/concentration when driving at night, or when an anomaly/risk presents itself suggesting an error is about to be made (among others). The common pattern is that some friction in System 1’s usual function is an invitation of System 2 to act.\nThere is one massive problem – System 2 is lazy! Given the choice, it errs on the side generally (although with considerable individual heterogeneity) towards inaction, e.g., it abides by the “law of least effort”. To appreciate this tendency towards a lazy state, refer back to the cognitive schematic – the control of the attention and working memory by System 2 for deliberative thinking requires effort and work. Thus both self-control and cognitive effort are forms of mental work. This leads to a phenomenon of ego-depletion: “activities that impose high demands on System 2 require self-control, and the exertion of self-control is depleting and unpleasant.” Difficult System 2 work, whether cognitive reasoning or self-control, in-fact consumes blood glucose.\nIn a remarkable study published in the Proceedings of the National Academy of Sciences on Israeli judges shows some perverse consequences of ego-depletion. In this legal setting, there are parole judges reviewing applications for parole, and each application is considered for 6 minutes each. The default decision is a denial (so they are deliberating to overturn a denial). On average 35% approved, however 65% approved immediately after a meal which dwindles to nearly 0% before next meal. The pattern is depicted in the findings from the paper below.\n\nThere is considerable individual heterogeneity in the natural tendency of System 2 to exert effort for executive control over System 1. Consider one of the more famous experiments in the history of psychology: the Walter Mischel cookie/marshmallow experiment\n\n\n\n\n\n\n\nThis suggests there are at least two different dimensions of System 2 discussed by Stanovich and West in their book Rationality and the Reflective Mind. One is intelligence – the sheer ability to excel in slow thinking and computation. The other is rationality, which is challenged by the degree to which System 2 wants to be lazy, which also varies at the individual level and features of what the authors call the reflective mind.\nThe effort expended by System 2 in relation to the effortless automation of System 1 provides some understanding as to why human cognition evolved this way? As TFS explains (p. 25)\n\nThe division of labor between System 1 and System 2 is highly efficient: it minimizes effort and optimizes performance. The arrangement works well most of the time because System 1 is generally very good at what it does: Its models of familiar situations are accurate, its short-term predictions are usually accurate as well, and its initial reactions to challenges are swift and generally appropriate. System 1 has biases, however, systematic errors that it is prone to make in specified circumstances. One further limitation of System 1 is that it cannot be turned off.\n\nThe interaction can be described graphically as followed\n\nMoreover this interaction is efficient - e.g., it minimizes effort to make approximately satisfactory judgments and decisions.\n\nThe difficulty arises when System 2, due to the underlying effort involved in its operation, does not “catch” an intuition or suggestion from System 1 that does not accord with the standards of reasoning of Homo Economicus! In that case, intuitions translate to beliefs, and suggestions into actions, and an inefficiency in human decisions will exist. We will examine next just how such beliefs come into being and operate faster than System 2 can often control.\n\n\n\n",
    "preview": "https://miro.medium.com/max/2400/1*kQdOtDv-7KolqkWPyQD0kw.png",
    "last_modified": "2021-02-10T22:05:15+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-04-human-centered-design/",
    "title": "Human Centered Design",
    "description": "Your semester project is a journey in designing a decision solution from a human perspective",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-02-04",
    "categories": [
      "Human-Centered Design",
      "Semester Project"
    ],
    "contents": "\nIntroduction\nOur previous posts and discussion have yielded 3 key realities for human decision making:\n Economists model human decisions using the predictions of Homo Economicus.  Homo Economicus exhibits at least seven central cognitive capabilities that are present in every decision it makes. The model straddles the line between a positive science and a normative science.\n Homo Economicus is a useful approximation, but there is more to human decisions than the mental operations of Homo Economicus.  Homo Economicus maps to our “slow thinking” selves - e.g., System 2. However System 1, our fast thinking selves, is in the drivers seat for many choices and decisions and there are systematic differences between the behavior of System 1 and Homo Economicus.\n The wedge between Homo Economicus and System 1 creates an opportunity to improve the efficiency of economic decisions in the real world.  However to influence behavior we will need to convince our “System 1” selves to act even if our “System 2” reasoning abilities are convinced.\nAs we will discover, one of the key limitations of System 1 is its challenge in processing data - there is a considerable distance between our “intutive statistician” selves and the formal statistical/econometric principles.\nData Science on the other hand is a tool for presenting and conveying the value of data to decision makers to influence their decisions.\nThis defines the scope of our semester projecr.\nWe will engage in a process of defining a decision problem, and iteratively working our way to data solution and and the creation of a simple mock up that creates value to a user, a human!\nTo design a solution that is useful to humans, we will thus engage in a process known as human centered design.\nThe process is laid out in the guidebook from IDEO.org, which we will be following, and the relevant sections posted on Perusall. The key steps are:\n\n\n\nA key aspect of the process the guidebook highlights is its non-linearity:\n\nHuman-centered design isn’t a perfectly linear process, and each project invariably has its own contours and character. But no matter what kind of design challenge you’ve got, you’ll move through three main phases: Inspiration, Ideation, and Implementation. By taking these three phases in turn, you’ll build deep empathy with the communities and individuals you’re designing for; you’ll figure out how to turn what you’ve learned into a chance to design a new solution; and you’ll build and test your ideas before finally putting them out into the world.\n\nAnother important part of the process is to work in teams and building quickly, before you know or understand exactly what you are building!\n\nTo maintain creativity and energy, we always work in teams. To keep our thinking generative, sharp, and because it helps us work things through, we always make tangible prototypes of our ideas. And because we rarely get it right the first time, we always share what we’ve made, and iterate based on the feedback we get.\n\nThe central teaching of the guidebook: Trust the process!\n\nHuman-centered design is a unique approach to problem solving, one that can occasionally feel more like madness than method—but you rarely get to new and innovative solutions if you always know precisely where you’re going. The process is designed to get you to learn directly from people, open yourself up to a breadth of creative possibilities, and then zero in on what’s most desirable, feasible, and viable for the people you’re designing for. You’ll find yourself frequently shifting gears through the process, and as you work through its three phases you’ll swiftly move from concrete observations to highly abstract thinking, and then right back again into the nuts and bolts of your prototype.\n\nThe journey is best described as a “converge-diverge” cycle, where you go big in your thinking, dial down into specifics and practicality, look back at the big picture, and iterate.\n\n\n\nMilestone 1: Framing a Design Challenge\nWe want to design solutions to improve a decision. The first step - arguably the hardest - is to define a problem or question that is worth solving! The guidebook refers to this as **Framing the Design Challenge*.\nYour task is to work with your team and frame a design challenge. For now you can imagine hypothesizing that there is a decision inefficiency that can be aided with appropriate application of data and imagine constructing a solution in an unconstrained way. The steps for this milestone are laid out as:\n\n\n\nYour can iterate on this process using the structure of the worksheet provided in the guidebook.\n\n\n\nYour final artifact for this milestone phase is a document that mimics this structure but adapted to the design problem of improving a decision - e.g., the problem you are solving is to improve a decision “in the wild” based on your perceived gap between the intuitive decisions and what “should” happen if our Homo Economicus selves could freely operate.\nWe will use google docs for this round and evolve our projects to Rmarkdown in subsequent phases. Your TA wil send links to each group for the link to a document where you can develop this phase’s milestone.\n\n\n\n",
    "preview": "posts/2021-02-04-human-centered-design/images/designlogo.png",
    "last_modified": "2021-02-04T15:03:54+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-02-02-friedmanarticle/",
    "title": "Friedman's Defense of Homo Economicus",
    "description": "The Methodology of Positive Economics",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [
      "Homo Economicus",
      "positive economics"
    ],
    "contents": "\nFriedman’s essay on the Methodology of Positive Economics written in 1953 is a classic exposition of the economic approach to studying problems. It forces us to contend with the the empirical interpretation of Homo Economicus and its role in the normative vs positive applications of economic theory.\nMilton Friedman\n\nBorn in 1912 in New York City\nBA Rutgers University(1932) MA University of Chicago \\(1933\\) PhD Columbia University \\(1946\\)\nProfessor at University of Chicago \\(1946\\-1977\\)\nFellow at the Hoover Institution,at StanfordUniversity \\(1977\\-\\)\nMentors:Arthur Burns, Wesley Mitchell, Jacob Viner, Frank Knight\n1976:Nobel Memorial Prize in Economics\nInfluence:in economics - New Classical Macroeconomics, in policy - Ronald Regan and, Margaret Thatcher \\(1980s\\)\nControversy:lecture in Chile in 1975\nHe died in San Francisco in 2006\nScientific contribution\nStatistics\nFriedman non-parametric tests JASA 1937, JASA 1939, AMS 1940\nMethodology\n“The Methodology of Positive Economics” in Essays in Positive Economics \\(1953\\)\nConsumption\nA Theory of the Consumption Function \\(1957\\)\nMonetary theory, history and policy\nA Monetary History of the United States, 1867-1960 with A. Schwartz, 1963;\n“The Quantity Theory of Money:A Restatement”, in Studies in the Quantity Theory of Money ed.M.Friedman,1956;\n“The Role of Monetary Policy”, (AER 1968); “A Theoretical Framework for Monetary Analysis” ( JPE 1970)\nBusiness cycles and inflation\n“Money and business cycles” with A. Schwartz (RES,1963)\nMonetary Trends in the US and the UK with A. Schwartz,1982;\n“Inflation and Unemployment” (JPE 1977)\nIdeology, Pop\nCapitialism and Freedom (1962)\nFree to Choose with R Friedman, 1980)\nThe “Normative” Friedman\nLater in Freidman’s career, he became a popuar voice for economics in the mainstream media, and would engage with large and broad audienecs “head-on” for live policy debates. He thus took a normative view of policy - what types of policies should be enacted - based on his understanding of the positive predictions from economic theory.\nIn addition to being a towering intellect, he was a fierce debater and orator of economics. Here is a famous discussion he gave on the power of prices in a market economy as a means to efficiently coordinated the multitude of economic decisions that are linked together in an economy.\n\n\n\n\n\n\n\nWe thus see his normative leanings towards market as drivers of overall efficiency - a topic which continues today to be hotly debated. In this normative stance, Homo Economicus was a means to an end - a way to reach conclusions about the effect of economic policies\nThe revival of the homo economicus\nIdea of homo economicus (Mill 1836; Robbins 1932)\nKeynes (1936): alternative views:\nanimal spirits\nfundamental psychological law\n\nFriedman:revivalof rational behaviour in economics\nconcept of “permanent income” in consumption\nrejection of permanent trade-off between inflationa ndunemployment due to expectations of future inflation\n\nMarshallian methodology\nFriedman was not as radical as the New Classical Macroeconomics (Lucas,Sargent):\n-rejection of Rational Expectation Hypothesis\nMarshallian vs. Walrasian methodology\nWalrasian perspective: general equilibrium and complete micro-foundation in individual optimization; secondary role of empirical evidence\nMarshallian perspective: partial equilibrium\ntheory as “an engine for the discovery of concrete truth” (Marshall1885)\nimportance of empirical economics\n\nFriedman considered himself a Marshallian\nF. 53\n“The Methodology of Positive Economics”\nA forceful, careful, and heavily influential essay on the meaning of the methodology of economics.\n CHICAGO. ECONOMICS 300A. CORE THEORY. GARY BECKER, 1956 \n\n_______________\n Economics 300A   Autumn 1956   Reading Assignments by G. Becker \nNOTES:\n1) A knowledge of the material in George Stigler,  A Theory of Price  or in Kenneth Boulding ,  Economic Analysis  , is a prerequisite.  2) Readings marked with an asterisk \\(*\\) are recommended, not required.\n I. INTRODUCTION \nFriedman, M.,  Lecture Notes  , pp. 1-16. Knight, F. H.,  The Economic Organization  , pp. 1-37. Friedman, Milton, “The New Methodology of Positive Economics,” in  Essays in Positive Economics  . *Hayek, F. A., “The Use of Knowledge in Society,”  American Economic Review  , September 1945, reprinted in  Individualism and Economic Order  . *Keynes, J. N.,  The Scope and Method of Political Economy  , pp. 1-83.\n II. DEMAND ANALYSIS \nMarshall, A.,  Principles of Economics  , Book III, chs . 2-4; Book V, chs . 1-2.\nF.’s enemies\n1946-1953: controversy on the “marginalist” theory of the firm\nHall and Hitch (1939): “Price Theory and Business Behaviour”, Oxford Economic Papers\nsurvey on how prices emerged within firms\nrules of thumb inconsistent with the hypothesis of maximization of expected profits\npsychological mechanisms different from model’s mechanism: false assumptions\n\nThe Rise of Chamberlainian and Walrasian Economics: Monopolistic Competition and General Equilibrium.\nFriedman is feeling the Marshallian approach under attack from multiple fronts. As his 1974 self revealed - how do you respond?\n“You cannot be sure that you are right unless you understand the arguments against your views better than your opponents do.”\nThe Methodology of Positive Economics (1953)\nOpening line:\nIn his admirable book on The Scope and Method of Political Economy, John Neville Keynes distinguishes among “a  positive  science . . . a body of systematized knowledge concerning what is;  a   normative  or regulative science ... a body of systematized knowledge discussing criteria of what ought to be . . . ; an  art  ... a system of rules for the attainment of a given end\"; comments that “confusion between them is common and has been the source of many mischievous errors\"; and urges the importance of “recognizing a distinct positive science of political economy”.\nHe thus raised the positive-normative distinction, which he continues to discuss\n\nNormative economics and the art of economics, on the other hand, cannot be independent of positive economics. Any policy conclusion necessarily rests on a prediction about the consequences of doing one thing rather than another, a prediction that must be based – implicitly or explicitly - on positive economics. There is not, of course, a one-to-one relation between policy conclusions and the conclusions of positive economics; if there were, there would be no separate normative science. Two individuals may agree on the consequences of a particular piece of legislation. One may regard them as desirable on balance and so favor the legislation; the other, as undesirable and so oppose the legislation. (page 5)\n\nThe Case for Positive Economics\nNormative versus positive economics\nPolicy questions center around “what policy should we as a society enact?”\nIn order to answer this we must be able to answer “if we enacted this policy what will happen?”\n\nI venture the judgment, however, that currently in the Western world, and especially in the United States, differences about economic policy among disinterested citizens derive predominantly from different predictions about the economic consequences of taking action - differences that in principle can be eliminated by the progress of positive economics - rather than from fundamental differences in basic values, differences about which men can ultimately only fight.” \\(page 7\\)\n\n\nThe ultimate goal of a positive science is the development of a “theory” or, “hypothesis” that yields valid and meaningful i.e., (not truistic) predictions about phenomena not yet observed. Such a theory is, in general, a complex intermixture of two elements. In part, it is a “language” designed to promote “systematic and organized methods of reasoning.” In part, it is a body of substantive hypotheses designed to abstract essential features of complex reality. \\(page 7\\)\n\nTranslation\nPositive Economics consists of two parts:\nA completely self-enclosed formal system consisting of assumptions and implications drawn from those assumptions\nThe hypotheses stated in the language of the science that abstracts “essential features” of the reality it purports to explain.\nPopperian perspective\nthe only relevant test of the validity of a hypothesis is comparison of its predictions with experience. The hypothesis is rejected if its predictions are contradicted “frequently” or more often than predictions from an alternative hypothesis; it is accepted if its predictions are not contradicted; great confidence is attached to it if it has survived many opportunities for contradiction. Factual evidence can never “prove” a hypothesis; it can only fail to disprove it, which is what we generally mean when we say, somewhat inexactly, that the hypothesis has been “confirmed” by experience\nDigression I: logical positivism\nOrigin in the Vienna circle of 1920s. Cfr. H.Feigl, R.Carnap, H.Reichenbach, M.Schlick\nSix tenets or tendencies (cfr.Hacking1983: 41-42):\nEmphasis upon verification\nPro observation\nAnti-cause\nDownplaying explanation\nAnti-theoretical entities\nTo sum up: against metaphysics!\n\nPopper’s variant: falsificationism\nPopper emphasized within this tradition two things:\nmodus tollens - If P implies Q and Q is false, then P is false. (related to _red uctio ad absurdum)\nSimplicity\nDuhem-Quine problem\n\nUnfortunately, we can seldom test particular predictions in the social sciences by experiments explicitly designed to eliminate what are judged to be the most important disturbing influences. Generally, we must rely on evidence cast up by the “experiments” that happen to occur…No experiment can be completely controlled, and every experience is partly controlled, in the sense that some disturbing influences are relatively constant in the course of it. (page 10)\n\nTranslation\nThe predictions that arise from economic theory are typically “ceteris paribus” statements.\nHolding all else equal, a change in X causes a change in Y#\nUnfortunately the data never “holds all else equal” – it is always full of “disturbing influences”\nThe inability to conduct “randomized experiments” makes these disturbing influences especially hard for taking theory to data.\n\nThe difficulty in the social sciences of getting new evidence for this class of phenomena and of judging its conformity with the implications of the hypothesis makes it tempting to suppose that other, more readily available, evidence is equally relevant to the validity of the hypothesis-to suppose that hypotheses have not only “implications” but also “assumptions” and that the conformity of these “assumptions” to “reality” is a test of the validity of the hypothesis different from or additional to the test by implications. This widely held view is fundamentally wrong and productive of much mischief. \\(page 14\\)\n\nThe F-twist\n Truly   important   and   significant   hypotheses   will   be   found   to   have   “assumptions”that   are   wildly   inaccurate   descriptive   representations   of   reality,   and,   in   general,   the   more   significant   the   theory,   the   more   unrealistic the   assumptions   in   this   sense.  The reason is simple. A hypothesis is important if it “explains”much by little, that is, if it  abstracts  the common and crucial elements from the mass of complex and detailed circumstances surrounding the phenomena to be explained and permits valid predictions on the basis of them alone. To be important, therefore, a hypothesis must be  descriptively   false  in its assumptions; it takes account of, and accounts for, none of the many other attendant circumstances, since its very success shows them to be irrelevant for the phenomena to be _explained_ (F53: 14-15, emphasis added)\nDigression II: realism vs. instrumentalism\nProblem of unobservables :lack of direct sensory access to theoretical entities in science (e.g.electrons,quarks,radiowaves,viruses,demandcurve,permanentincome,velocityofmoney)\nRealism : scientific theories \\(should\\) deliver true description of the world (or of some structure of the world),including the unobservable part\nemphasis on description and explanation\n\nInstrumentalism : we are in no position to get true descriptions of the world: theories are useful instruments to generate predictions\nemphasis on prediction and manipulation\n\nRealism vs. realisticness\nIn economics it is particularly important to distinguish realism ,which emphasizes the possibility of identifying faithful description of some structures of reality,from the degree of accurateness of the description.\nUskali Mäki (1994, 1998) calls descriptive accuracy realisticness\nThus one can be realist with respect to some model or theory without emphasizing its realisticness\nThe importance of abstractions and idealizations\nF. on the realism of the assumptions\nthe relevant question to ask about the “assumptions” of a theory is not whether they are descriptively “realistic,” for they never are, but whether they are sufficiently good approximations for the purpose in hand. And this question can be answered only by seeing whether the theory works, which means whether it yields sufficiently accurate _predictions. (F53:15)\nFirst example: Galilean experiment\nIt is an accepted hypothesis that the acceleration of a body dropped in a vacuum is a constant - g, or approximately 32 feet per second per second on the earth - and is independent of the shape of the body, the manner of dropping it, etc. This implies that the distance traveled by a falling body in any specified time is given by the formula s = (1/2)gt^2 , where s is the distance traveled in feet and t is time in seconds. The application of this formula to a compact ball dropped from the roof of a building is equivalent to saying that a ball so dropped behaves  as   if  it were falling in a vacuum. Testing this hypothesis by its assumptions presumably means measuring the actual air pressure and deciding whether it is close enough to zero. (F53: 16-17, emphasis added)\nThis example illustrates both the impossibility of testing a theory by its assumptions and also the ambiguity of the concept “the assumptions of a theory.” The formula s =(1/2) gt^2 is valid for bodies falling in a vacuum and can be derived by analyzing the behavior of such bodies. It can therefore be stated: under a wide range of circumstances, bodies that fall in the actual atmosphere behave as if they were falling in a vacuum. The formula is accepted because it works, not because we live in an approximate vacuum - whatever that means. (F53: 17-18)\nTranslation\nA Galilean Assumption: Air Pressure = 0\nRole of assumption: All forces other than gravitation = 0\nIdealizes gravity as the sole cause of the motion of a falling body, assuming all other forces are powerless.\nA Neoclassical Assumption: Producers and traders pursue maximum expected returns\nRole of assumption: All other motives except the maximization motive have zero strength.\nSecond example: an evolutionary argument\nConsider the density of leaves around a tree. I suggest the hypothesis that the leaves are positioned  as   if   each   leaf   deliberately   sought   to   maximize   the   amount   of   sunlight   it   receives  , given the position of its neighbors,  as   if   it   knew   the   physical   laws  determining the amount of sunlight that would be received in various positions and could move rapidly or instantaneously from any one position to any other desired and unoccupied position. Now some of the more obvious implications of this hypothesis are clearly consistent with experience: for example, leaves are in general denser on the south than on the north side of trees but, as the hypothesis implies, less so or not at all on the northern slope of a hill or when the south side of the trees is shaded in some other way. ... the hypothesis does not assert that leaves do these things but only that their density is the same  as   if  they did. Despite the apparent falsity of the “assumptions” of the hypothesis, it has great plausibility because of the conformity of its implications with observation. (F53: 19-20, emphasis added)\nThird example: the billiard player\nConsider the problem of predicting the shots made by an expert billiard player. It seems not at all unreasonable that excellent predictions would be yielded by the hypothesis that the billiard player made his shots  as   if  he knew the complicated mathematical formulas that would give the optimum directions of travel, could estimate accurately by eye the angles, etc., describing the location of the balls, could make lightning calculations from the formulas, and could then make the balls travel in the direction indicated by the formulas. (F53:21)\n\nNow, of course, businessmen do not actually and literally solve the system of simultaneous equations in terms of which the mathematical economist finds it convenient to express this hypothesis, any more than leaves or billiard players explicitly go through complicated mathematical calculations… The billiard player, if asked how he decides where to hit the ball, may say that he “just figures it out” but then also rubs a rabbit’s foot just to make sure; and the businessman may well say that he prices at average cost, with of course some minor deviations when the market makes it necessary. The one statement is about as helpful as the other, and neither is a relevant test of the associated hypothesis.\n\nBottom line\nindividual firm behave as if they were seeking rationally to maximize their expected returns ...and had full knowledge of the data needed to succeed in this attempt; as if, that is, they knew the relevant cost and demand functions, calculated marginal cost and marginal revenue from all actions open to them, and pushed each line of action to the point at which the relevant marginal cost and marginal revenue were equal. (F53:21)\nF.’s ambiguities\nIt would not be fair to dub F. as an instrumentalist\nHe swings between realism (but anti-realisticness) and instrumentalism\nEspecially if we judge F53 vis-à-vis Friedman and Schwartz (1963) and his professed Marshallian methodology\nMarshall took the world as it is; he sought to construct an “engine” to analyze it, not a photographic reproduction of it (F53:35)\nConclusion\nThe relevant hypothesis is: “I can treat the data of interest AS IF it were generated by the ideal types contained in my model”.\nA reasonable starting point if model was generated by isolating essential features of reality in the first place.\nHow can we test our model of Homo Economicus\nImagine the following question was asked to a decision maker:\nYou won a free ticket to an Eric Clapton concert.\nBob Dylan is playing on the same night and is your most attractive alternative.\nTicket to see Dylan is $40\nYou would be willing to pay up to $50 to see Dylan\nNo other costs of seeing either performer\nWilling to pay $50 to see Dylan\nCost of Dylan ticket = $40\nBased on this information, what is the opportunity cost of seeing Clapton?\n$0\n$10\n$40\n$50\n7.4% of 270 undergrads who had previously taken a course in economics answered the question correctly\n17.2% of 88 undergrads who had never taken a course in economics answered the question correctly\nWhat would Friedman say to this data as a test against Homo Economicus\n\n\n\n",
    "preview": "http://c250.columbia.edu/images/c250_celebrates/remarkable_columbians/240x240_bio_friedman.jpg",
    "last_modified": "2021-02-02T13:25:15+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-28-visualizations/",
    "title": "Some Famous Data Visualizations",
    "description": "A look at some visualizations from the pre-digital era",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": {}
      }
    ],
    "date": "2021-01-28",
    "categories": [],
    "contents": "\nNapolean’s March\nDesigner: Charles Joseph Minard\n\nThe Cholera Map\nDesigner: John Snow\n\nWorld War II Fighter Planes\nDesigner: Abraham Wald\n\n\n\n\n",
    "preview": "https://cdn8.openculture.com/2019/07/11094725/Minard-1-e1562863679105.png",
    "last_modified": "2021-01-28T17:47:19+00:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "An Introduction to Homo Economicus",
    "description": "Who is Homo Economicus and how is he/she related to you?",
    "author": [
      {
        "name": "Amit Gandhi",
        "url": "upenn.edu"
      }
    ],
    "date": "2021-01-28",
    "categories": [
      "Data",
      "Cloud",
      "Homo Economicus",
      "Behavioral Economics"
    ],
    "contents": "\n\nContents\nIntroduction\n(Re-)Introducing Homo Economicus\nHow did Homo Economicus evolve?\nLets try an Example\n\n\nKey Assumptions\nWhat are the traits of Homo Economicus\nBut You are Actually Homo Sapien\n\nWhat should be done?\nA question\n\nIntroduction\n\n\n\n\nLife is a matter of choices, and every choice you make makes you.\n—John C. Maxwell\n\nEconomics is the science of choices - choice under scarcity specifically. Of course given that all choices involve an opportunity cost - the value of your next best alternative to the choice that was actually made - all choices are made in the presence of scarcity. Or said another way, in the words of a famous economic theorem you have likely heard - there is no free lunch!1\nThe way economics approaches this science is actually a little quixotic and takes some time to appreciate the angle of attack. In order to understand the choices made by actual people in an economic system, an economic model will typically start with asking what may seem like an unusual question: what would a hypothetical rational actor choose under the circumstances faced by actual person or entity making the choice?\nNotice that this is distinct from asking : “what would the actual person or entity in question choose?”. The reason for the difference is that the “rational actor experiment” - embedding a hypothetical rational actor in the setting faced by an actual decision maker - provides insight into the major forces that determine the outcome of the decision. By mapping the cause and effect relationship from decision forces to behavior, it enables economists to predict what different decisions could result as those forces are altered.\nA key force of interest in economics are incentives. Hence the rational actor is not meant to be a perfect replication of the entity in question in question making the decision, but rather a model that provides an approximation and is designed to isolate the effect of incentives on behavior.\nThis actor interestingly enough has a name: Homo Economicus. It is a whole species of humanity that abides by the axioms of rational choice!\n\nHomo Economicus\n(Re-)Introducing Homo Economicus\nIf you don’t remember your Homo Economicus, lets have a quick refresher. Consider the list of options below which represent potential payoffs from choosing that option. Which option would you choose.\n\n\n\nThe answer should likely jump off the page. The option 20 feels (and indeed is) the highest payoff, and given that all else equal you prefer more to less, you choose it.\nIf you have chosen in this fashion, there is a Homo Economicus living inside of you. You have entered a choice situation with a clear objective - maximize payoff. Yo also likely delibratively considered each option and assessed the one which best fulfilled your objective. In other word, you have just maximized the utility subject to a budget constraint - a key capability of Homo Economicus!\nHow did Homo Economicus evolve?\nHomo Economicus was historically discovered not through laboratory experiments, but rather through a process of human introspection. A collection of scholars spanning multiple fields - ranging from Economics, Mathematics, Statistics, Computer Science, Operations Research, as well as Psychology and Sociology and even Philosophy - have all contributed to the modern conception of Homo Economicus by asking a common question: How should a rational actor behave given the logical structure of a decision problem.\nThis introspective methodology created the basic DNA of Homo Economicus. This DNA has evolved from the 18th century enlightenment thought to the present day in a fashion akin to natural selection as a wider variety of decision problems confronted Homo Economicus. These problems arose from a practical need to solve economic questions as opposed to completely abstract undertakings - e.g., they were problems truly found in the wild of applied economics.\nHomo Economicus is thus a human creation - a model built by humans for humans to shed light and understanding on the human condition.\nHomo Economicus HistoryLets try an Example\nDecision Maker Doris who lives for two periods\nIn each period she will receive some income.\nIn the first period does not know what income in second period will be\n\nIn each period will spend money on two goods\nBourbon \\(b\\) or Yoga classes \\(y\\)\nShe has utility \\(u(b,y)\\) from consuming these two goods in any period she consumes them\n\nCan borrow and save between periods at interest rate \\(r\\), but cannot die in debt\nOur job (as economists) is to predict how Doris is going to behave -How much bourbon and yoga she will she consume in each period? -How much will she save?\nHow would Homo Economicus solve this decision problem?\nKey Assumptions\nDoris makes optimal choices – i.e. maximizes her utility\nDoris’s tastes do not change between period 1 and 2\nDoris forms the correct expectations about income in period 2, and makes choices based on expected utility\nThe only thing that appears in Doris’s utility function is the amount of bourbon and yoga consumed -No ‘reference points’ -Not other people’s consumption\nDoris makes optimal choices – i.e. maximizes her utility\nDoris’s tastes do not change between period 1 and 2\nDoris forms the correct expectations about income in period 2, and makes choices based on expected utility The only thing that appears in Doris’s utility function is the amount of bourbon and yoga consumed -No ‘reference points’ -Not other people’s consumption\nWhat are the traits of Homo Economicus\nThe core anatomy of Homo Economicus that has emerged in modern economics encompasses the following set of traits:\nHas stable preferences\nMaximizes utility\nForms beliefs about uncertain outcomes satisfying laws of probability distributions (probablistic sophistication)\nUpdates beliefs with new information through Bayes Rule\nMakes decisions under uncertainty according to the principle of expected utility\nMakes decision over time in accordance with Bellman’s principle (dynamically consistent).\nThese traits are able to mix and match together depending on the structure of the choice problem to give rise to a very general model of rational behavior that is highly adaptable to most decisions. One of the first skills taught to graduate students in economics is to build a Homo Economicus in problem sets and computer simulations and to observe its behavior!\nBut You are Actually Homo Sapien\nAll of this is great for the economics profession by what does it matter to you. Consider the following variation of the choice problem we presented above. What choice would you make among the following options.\n\n\n\nThis choice problem feels different and triggers a rather different thought process. It is harder and your conclusion is more uncertain although there is no uncertainty in the question. Likely you will start to do a few operations of addition which you full well understand how to do in theory but don’t particularly enjoy in practice. You will grow slightly weary of this repetition (different individuals will have a different threshold for this pain), you will then resort to glancing through the patterns of the numbers in the numbers and “recognize” through an intuitive sense which is the highest payoff from among these based on this pattern.\nIf you functioned in this way, then congratulations you are indeed human - i.e., Homo Sapien! Homo Sapien, unlike Homo Economicus, is governed by another “thinking and reasoning system” that allows you to quickly assemble data from the environment, make sense of it, draw conclusions, and make decisions in a fairly automatic fashion. This thinking system stands at the front line of many decisions you actually make. It is distinguished by its remarkable speed and fairly effortless operation.\nFor Homo Economicus the two presentations of the decision problem yield the exact same choice - the the problems are logically identical becauset the choice options themselves are mathematically equivalent. However for Homo Economicus they lead us down different “thinking” pathway. The end result of the two pathways lead to materially different decisions.\nConsider the following data from an experiment conducted in 2011 by Caplin, Dean, and Martin (see here), they presented 22 Subjects with 657 choices analgous to the choices above but the choice problem varied among subjects in two key dimensions:\n2 complexity levels: 3 or 7 operations\n3 choice set sizes: 10, 20, 40 options\nwhich gave 6 overall treatments. An example of a choice problem can be seen below: \nThe results of the study are found below:\n\n\n\nThe outcome reveals a few key realities that will be central themes for us:\n1) Homo Sapien falls short of the rational ideal of Homo Economicus - Homo Sapien is boundedly rationality. \n2) The more complex environment exacerbates the difference with considerable heterogeneity among individuals. \n3) Homo Sapien’s choices are however reasonably effective - we leave cash on the table, but the level of inefficiency experienced is perhaps acceptable given the onerous calculations that would have been involved.\nWhat should be done?\nHere is where our chief quandary will lie.\nEconomists for the most part build fairly sophisticated models of Homo Economicus through data, theory, and econometric technique which are motivated by real world problems faced by actual economic actors.\nBut at the end of the day you on the other hand are Homo Sapien. We are left with two options to then consider:\nOption A:   Economists should really be performing genetic engineering on Homo Economicus to look and act more like Homo Sapiens. In this way build more realistic models of human behavior.\nOption B:   Homo Sapien should act more like Homo Economicus if indeed the latter is rational and no one is likely to dispute a basic desire to be rational.\nBoth options are interesting and quite important - there is an opportunity to build more predictive/realistic economic models and at the same time guide Homo Sapien towards a state of heightened rationality in actual behavior.\nYet neither option if tenable if the difference between Homo Sapien and Homo Economicus and Homo Sapien is due to the whims and randomness of passion. That is, if the dividing line between Homo Economicus and Homo Sapien is attributable to a third specimen of being - Homo Emotionalis.\n\n\n\nThis point of view was the established wisdom in the social sciences for the better part of the 20th century. The relevant departures from rational choice where the outcome of emotional reactions to the environment - the behavior of Homo Emotionalis.\nUnder this paradigm, the general predisposition towards Homo Economicus was the norm, with the countervailing influence of an emotions that leading to momentary but significant departures from the rational norm.\nMathematically we can imagine a theory of behavior under this view of people that resembled:\n\\[\n\\color{blue}{\\mbox{actual behavior} = \\mbox{rational behavior} + \\epsilon}\n\\]\nwhere \\(\\epsilon\\) is a random factor that causes our real behavior to fluctuate from the tenants of rational choice.\nUnder this model, our rational selves are at the center and the emotional perturbations are disruptions to the center. This model\nThis rational system of thought which you are capable of internalizing we will identify with a dimension of your brain following Kahneman as your “System 2”. The other designations we can give System 2 are your deliberate, rational, and formal thought system.\nThe view emerging from System 1 and System 2 is actually \\[\n\\color{blue}{\\mbox{actual behavior} = \\mbox{intuitive behavior} + \\mbox{rational adjustmnet}}\n\\]\nSystem 2 is your inner Homo Economicus. It can follow the precepts of formal reasoning and motivate action on that basis. It is however lazy. It takes cues from system 1. All else equal it would rather outsource its job to another complementary This intuitive system of thought we will represent as another dimension of your brain that (following Kahneman) will call “System 1”.\nNow we recognize that System 1 and System 2 are both present at the moment we are making a decision. Our main text in the class - thinking fast and slow - will be focused on the dual operation of System 1 and System 2 and how they operate together in a rich interplay that guides human decisions to predictably depart from Homo Economicus.\nA question\nWhy don’t we just take classes in decision making to solve the problem?\n\nThe choices are made by a set of economic actors whose decisions interrelate to give rise to the fundamental economic outcomes in society. This includes what gets produced, how is it produced, and who receives it. The underlying actors themselves encompass consumers/households, firms/organizations, governments, and the market mechanism itself - e.g., the invisible hand.↩︎\n",
    "preview": "https://theprogenygroup.com/wp-content/uploads/2016/06/homo-economicus.jpg",
    "last_modified": "2021-02-10T22:09:25+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-26-perusall-information/",
    "title": "Perusall Information",
    "description": "A short description of how Perusall works",
    "author": [
      {
        "name": "Amit K. Gandhi",
        "url": "upenn.edu"
      }
    ],
    "date": "2021-01-26",
    "categories": [],
    "contents": "\n\nHow Perusall Works\nPerusall helps you master readings faster, understand the material better, and get more out of your classes. To achieve this goal, you will be collaboratively annotating the textbook with others in your class.\nThe help you’ll get and provide your classmates (even if you don’t know anyone personally) will get you past confusions quickly and will make the process more fun. While you read, you’ll receive rapid answers to your questions, help others resolve their questions (which also helps you learn), and advise the instructor how to make class time most productive.\nYou can start a new annotation thread in Perusall by highlighting text, asking a question, or posting a comment; you can also add a reply or comment to an existing thread. Each thread is like a chat with one or more members of your class, and it happens in real time. Your goals in annotating each reading assignment are to stimulate discussion by posting good questions or comments and to help others by answering their questions.\n\n\n\n\n\n\n\n\n\n\n\nResearch shows that by annotating thoughtfully, you’ll learn more, so here’s what “annotating thoughtfully” means: Effective annotations deeply engage points in the readings, stimulate discussion, offer informative questions or comments, and help others by addressing their questions or confusions. To help you connect with classmates, you can “mention” a classmate in a comment or question to have them notified by email (they’ll also see a notification immediately if online), and you’ll also be notified when your classmates respond to your questions.\nFor each assignment, the application will evaluate the annotations you submit on time (see below). Based on the overall body of your annotations, you will receive a score for each assignment as follows\n\n3 = demonstrates exceptionally thoughtful and thorough reading of the entire assignment\n2 = demonstrates thoughtful and thorough reading of the entire assignment\n1 = demonstrates superficial reading of the entire assignment OR thoughtful reading of only part of the assignment\n0 = demonstrates superficial reading of only part of the assignment\n\nHow many annotations do I need to enter?\nWhen Perusall examines your annotations, its “assessment engine” has been designed to reflect the effort you put in your study of the text. It is unlikely that that effort will be reflected by just a few thoughtful annotations per assignment. On the other extreme, 30 per assignment is probably too many, unless a number of them are superficial or short comments or questions (which is fine, because it is OK to engage in chat with your peers). Somewhere in between these two extremes is about right and, thoughtful questions or comments that stimulate discussion or thoughtful and helpful answers to other students’ questions will earn you a higher score for the assignment.\nWhat does “on time” mean?\nThe work done in class depends on you having done the reading in advance. We will use the data arising from student responses on Perusall reading assignments as a guide for classroom discussion. Hence it is necessary to complete the reading and post your annotations before the deadline to receive credit. The deadline is a hard deadline, typically set 11pm on the night prior to the day of the lecture the reading is discussed.\n\n\n\n",
    "preview": "https://lms.unimelb.edu.au/__data/assets/image/0004/3355654/tile-perusall.png",
    "last_modified": "2021-01-26T15:09:59+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-25-course-introduction/",
    "title": "Course Introduction",
    "description": "Economics at the Intersection of Data and Technology",
    "author": [
      {
        "name": "Amit K. Gandhi",
        "url": "upenn.edu"
      }
    ],
    "date": "2021-01-25",
    "categories": [
      "Data",
      "A.I.",
      "Homo Economicus",
      "Behavioral Economics"
    ],
    "contents": "\n\nContents\nWelcome\nWhat is Market Design?\nWhat is the Topic of Our Class?Our approach to market design\nHow does this differ from data science?\n\nHow We Will LearnReading (50%)\nProject (50%)\n\nCourse Logistics\nCourse ScheduleWeek 01, 01/18 - 01/22\nWeek 02, 01/25 - 01/29\nWeek 03, 02/01 - 02/05\nWeek 04, 02/08 - 02/12\nWeek 05, 02/15 - 02/19\nWeek 06, 02/22 - 02/26\nWeek 07, 03/01 - 03/05\nWeek 08, 03/08 - 03/12\nWeek 09, 03/15 - 03/19\nWeek 10, 03/22 - 03/26\nWeek 11, 03/29 - 04/02\nWeek 12, 04/05 - 04/09\nWeek 13, 04/12 - 04/16\nWeek 14, 04/19 - 04/23\nWeek 15, 04/26 - 04/30\nWeek 16, 05/03 - 05/07\n\n\n\n\n\n \n \n Course \nEcon 262\n Professor\nAmit Gandhi (akgandhi@upenn.edu)\n TA\nNawaaz Khalfan (khalfan@sas.upenn.edu)\n Lecture \nTues, Thursday 1.30-2.50\n Office Hours \nBy appointment\n Website \nmarketdesign.io\n\n\nWelcome\nWelcome to Market Design (Econ 262)! This is a virtual course in the economics of decision making being taught in the Spring of 2021 at the University of Pennsylvania. This page describes the topic of the course, the learning objectives, the logistical plan, the evaluation policies, and the class schedule.\nThe course website will act as a central repository for the class to disseminate lecture materials and announcements. A GroupMe group for the class has been setup (reach out to Amit for access if you do not have it yet) and will serve as our platform to communicate with each other, conduct flash polls, and send reminders of class activities (including links for zoom class meetings.)\nWhat is Market Design?\nThe goal of market design is to design policies that improve the efficiency of economic decisions. An economic decision is any decision that involves trade-offs, which is synonymous with the very definition of economics as “the study of the allocation of scarce means to satisfy competing ends.” (Becker 2017)\nEconomic decisions are made by diverse actors in an economic system - households, firms, governments, and even the markets themselves (e.g., the invisible hand!).\nAn economic decision is efficient if it makes an optimal trade-off given the objectives and constraints of the decision maker.\nHelping make decisions more efficient creates value, generates profits, and raises welfare. But how should such interventions be designed? This is the central problem of market design.\nWhat is the Topic of Our Class?\nOur class is focused on the problems of market design in digital environments.\nHumans are increasingly making decisions in the presence of screens and devices, both at work and home. This has been especially pronounced during the last year of the pandemic where life quite literally moved online!\n\nA digital environment is one where computers and devices are networked together to facilitate human communication and transactions.\nWhat is the impact of digital environments on economic decisions? Does the transition from offline to digital environment improve economic efficiency? Can digital environments be designed to enable humans to better optimize their economic decisions?\nThese are central questions for market design in digital environments.\nA key difference between digital decisions and their offline counterparts is the role of data. A digital decision can leverage unprecedented velocity, variety, and volume  of data. The proliferation of data is a product of technological advancements in data science, data engineering, data analytics, machine learning, and A.I, which are distributed to decision makers through public cloud infrastructure.\nA fundamental question is then:\n\nMore Data = Better Decisions?\n\nThe answer is not as straightforward as it may seem. From a purely statistical or econometric viewpoint - more data is always better - we get better parameter inferences, better predictive accuracy, and better power to test models.\nHowever from a decision making perspective matters are less clear. The main problem is that data and A.I. - despite the hype - is not a panacea by itself. The technologies alone do not magically transform decisions. As economists are fond of saying, there is no free lunch!\nHal Varian (Chief Economist at Google and Professor at Berkeley) summarized the matter nicely in a recent interview:\n\nI think there’s a mystical belief in the power of data. Data is like oil in one respect… namely, it needs to be refined in order to be useful. So the data itself is not the important components, the know-how to refine it into something that’s useful. It’s the same [when] we talk about oil or data – it’s just the raw material, it’s not the finished product.\n\nIn order to create value, data technologies need to be transformed into applications that solve real problems and improve economic decisions. That is, applications need to be designed to extract value from data. Without good design, we are left with an alternative possibility:\n\nMore Data =  More Complexity =  Worse Decisions!\n\nA  central problem  for market design is  designing digital environment to translate “more data” into “better economic decisions”..\nOur approach to market design\nHow should digital environment be designed to enable efficient economic decisions?\nThe first rule of good design is to know your audience. For whom are we designing? Who is the user?\nLet me state the obvious answer which nevertheless has some profound implications for market design. The user is a human being!\n\nYou cannot understand good design if you do not understand people; design is made for people.\n\n— Dieter Rams\n\nOur approach in the class is to start with the user - the human - and understand how humans process and act upon data for economic decision making.\nHumans interact with raw data in a digital environment typically through a web application. The application processes, models, summarizes, and displays features of the raw data to the user, who internalizes the information in their behavior.\n\nThus from a user perspective, effective design in digital environments requires that a market designer to do 3 things:\n1. Identify an existing inefficiency in a human decision making process.\n2. Improve the the decision by leveraging data, technology, and economics.\n3. Influence humans to change their behavior toward the efficient outcome through digital interfaces.\n\nThe design problem encompasses all three steps. We can see that success in a market design project involves holistic thinking that integrates data/economic techniques with the visual elements of the digital interface, which are all tailored to the human behaviors we aim to effectuate and improve.\nThis is hard! It is a domain that is still in its infancy and being developed in real time across academic, business, and government as organizations digitally transform make data more central to their operation.\n\nHow does this differ from data science?\nA standard data science or machine learning classes will focus attention on the elements (2) (usually substituting “better decisions” with “better predictions”). These classes will focus on software techniques and algorithms for building and deploying ML algorithms.\nWe are instead interested in the underlying human decision problems that these algorithms and ultimately the data are intended to improve. Thus our focus is defining the problem (1) and generating influence (3), which provides the perspective necessary for solving (2).\nThus our focus is the human-centered design of data technologies to enhance economic decisions.\n\nI recognize that combining “human-centered” and “economic” together is not often how the “dismal science” is practiced or perceived. 😄\nHow We Will Learn\nIn order to solve the design problem above, we need to understand our user - human decision makers - in the wild. The investigation will proceed along two parallel tracks of learning in the class:\nReading\nCourse Project\nEach will count 50% towards your final course evaluation.\nThere is no curve in this class and hence no quotas on A’s or C’s. If you earnestly attempt to think and understand down each of these tracks, the metrics we have set up (and described below) will detect it and you will do well.\nReading (50%)\nA core learning experience in the course is rooted in reading the assigned book chapters and papers, and discussing them as a class. We will add a technological twist to this age old formula by adopting the  Perusall  application.\nOur Perusall code for the class is GANDHI-HUX9J. Go to to the Perusall website and enter this code to access our class library where all reading assignments will be posted.\nPerusall manages your engagement in the weekly readings. Perusall itself is based on an intelligent data technology that is aiding a key form of decision making in the classroom - student evaluations! Perusall measures engagement by your annotations in the reading, which forces you into an active reading mode through similar social forces that motivate our online media habits - it thus also embodies key features of Nudge (one of the books below)!\nOur readings fall into two categories:\nReading to gain perspective on human biases in decision making relative to efficient economic norms, and the effect of data on the manifestation of those biases.\nReading to learn tools for understanding human users and influencing them with the data scientific artifacts.\nWe describe the main texts that are part of the reading:\nThinking Fast and Slow\n\nIn order to identify and influence economic inefficiencies in the wild, we need a map to know where to look. If human departures from efficient decisions happen completely at random, then the search would be difficult if not impossible - we would be at the mercy of luck to successfully design!\nFortunately this is not the case. What is arguably the biggest discovery in the social and behavioral science in the last 50 years is that there are robust patterns of human cognition that make humans, e.g, Homo Sapien behave differently than the rational ideal, e.g., Homo Economicus.\nThe science we will apply to the question follows from the seminal work of Kahneman and Tversy (KT for short) who in 1971 began the first inquiry into the question of whether humans naturally are intuitive statisticians - e.g., do humans instinctively process data in a fashion that abides by formal statistical principles?\n\nThe surprising answer to the question was “no”, and led to an influential research paradigm known as “Heuristics and Biases” (H&B for short).\nThe crux of H&B, which is among the biggest discovery in the social and behavioral sciences of the last 50 years, is that there are robust patterns of human cognition that make humans, e.g, Homo Sapien behave differently than the rational ideal, e.g., Homo Economicus. The program spawned the field of behavioral economics and led to Kahneman being awarded the Nobel Prize for Economic Science in 2002 (Tversy sadly passed away in 1996).\nHomo Economicus is the \"rational actor&quot from your principles of economics courses used to model decisions by consumers, firms, and societies/governments.\nThe book “Thinking Fast and Slow” by Daniel Kahneman - describes the conceptual and historical backdrop behind H&B and provides the basic clues for where we can anticipate that sub-rational economic decisions will happen. Identifying these scenarios is a central part of being a market designer in the digital age - data and AI when properly employed should ameliorate these biases and improve economic efficiency.\nNudge (optional)\n\nIdentifying an inefficiency in an economic decision is one thing. Changing someone’s behavior towards a more efficient state is quite another. No one likes to be told what to do, especially from data they may not fully be aware or understand.\nPerhaps the answer lies not in directing the outcome of the decision, but rather to nudge the decision maker towards the rational outcome while respecting their autonomy over the decision.\nThis is the key idea behind the book Nudge by Richard Thaler and Cass Sunstein. Richard Thaler is a behavioral economist who developed the first economic applications of Kahneman and Tversky’s discoveries. He was awarded the Nobel prize himself in 2017.\nAn implication of H&B for market design is that it is not simply what data is presented to humans, but how they are presented that affects which economic decisions are made. It implies that both the presentation of information as well as the information itself are part of the design problem. Said another way - The interface matters!\nNudge applies this fact and recognizes that interfaces (what they call choice contexts) can manipulated and hence designed to produce decisions that are more rational. Thaler and Sunstein call this type of market design choice architecture - designing the choice environment to nudge people to behave more rationally without having to make people “think” more rationally!\nHowever the practice of designing nudges has a potential dark side - especially in digital environments. They can be used to exploit human frailties to extract profits in a way that may not be human welfare improving to the user. Of course the definition of welfare itself can be in the eye of the beholder, and the dividing line between good vs evil nudges is not always clear. We will aim to be mindful of this important complexity as we explore the topic.\nR for Data Science\n\nBoth “Thinking Fast and Slow” and “Nudge” were written before data science came of age. In fact the term “data science” itself was coined in 2008 (by D.J. Patil and Jeff Hammerbacher), the same year as Nudge was published.\n\nData science is in many ways represents the modern day choice architecture. Data scientist in organizations across industries are processing, filtering, and designing displays of information that defines the context for a large variety of our economic decisions.\nData science should be seen as both a set of tools for working with data, as well as a process for answering questions and solving problems with data to influence stakeholders seeking value from data (typically decision makers who are the clients or customers of a data analysis). The book R for Data Science written by Hadley Wickham has become a ``modern classic’’ in establishing an elegant and powerful set of tools and processes.\nThe tools he describes, including ggplot, dplyr, and tibbles, are the staples of being a productive data scientist. The process, which sometimes receives less attention, is just as powerful and encapsulated by the following data science workflow: \nThe tidyverse is a collection of packages that have a common set of design principles and interfaces that juxtaposes against this workflow to create a rich ecosystem for data science. A sampling of the most popular packages can be seen here: \nParticularly of interest for digital design of decisions is the communication step. As can be seen, communication is the last mile of the data scientific workflow and it is arguably the most important (and unfortunately least taught!). The communicaton step is associated to a specific tooling element: rmarkdown.\nRmarkdown weaves together code, text, and visualizations in a single software medium intended for human consumption. It lives as a text file, e.g., a piece of code, which can be versioned and generates a communication that is reproducible. We will examine rmarkdown and the process of building data based communications that can influence the behavior of human decision makers.\nProject (50%)\nYou will work in teams of 3-4 and engage in a human centered design process that applies data to help solve a decision problem. The process will entail 7 distinct stages that you will carry your project forward with your team. The stages correspond to the elements of a human centered design process for data solutions:\n\nProject Flow\n\n\n\n{\"x\":{\"diagram\":\"digraph{\\n\\n                     graph[rankdir = TD]\\n                     \\n                     node[shape = rectangle, style = filled, fontsize = 44]  \\n                     A[label = \\\"Decision Problem\\\"]\\n                     B[label = \\\"User Persona\\\"]\\n                     C[label = \\\"User Interview\\\"]\\n                     D[label = \\\"Data Collection\\\"]\\n                     E[label = \\\"Construct Prototype\\\"]\\n                     F[label = \\\"Collect Feedback\\\"]\\n                     G[label = \\\"Iterate and Finalize\\\"]\\n\\n                     edge[color = black, fontsize = 44]\\n                     A -> B\\n                     B -> C\\n                     C -> D\\n                     D -> E\\n                     E -> F\\n                     F -> G\\n                     \\n                     }\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\nThe components are:\n1: Define a decision problem of interest/concern 2: Hypothesize user personas and recruit users for interviews 3: Conduct user interviews and test/iterate the problem and persona definitions 4: Collect data for the problem 5: Construct a prototype 6: Collect user feedback on the prototype 7: Iterate and finalize prototype \nYour journey along this path will documented in a team blog. Each stage is a milestone in the project. Your output from each stage will correspond to a blog post. We will use rmarkdown for creating team blogs. Instructions for building your blog with Rmarkdown will be provided in the class.\nThe goal of the project is to get you to experience first hand what I view as the most challenging aspect of the design problem. This is usually not the technical AI, ML, or Data Science component, but rather understanding the user experience and user value for the decision being improved. The particular data technique that factors into building a prototype can be very simple - a well designed data visualization for example can more than suffice for a successful prototype, but it is the discretion of the team to find the appropriate data design for the problem at hand.\nYou are not being evaluated on the intricacy of the prototype- there is no failure! Your evaluation is based on attempting to follow the process to develop a useful scenario for the application of data to the decision problem you have scoped. A big part of success is thus having a well defined problem, which itself will feel ambiguous and will morph as you go along the process.\nThus you should see the process as an education in itself in real world design of data solution. You will struggle with ambiguity and test ideas, and discard many false starts before you find a path towards a solution that adds value to a decision problem. This is the nature of the iterative method and keeps you constantly connected to the user scenario before investing heavily in the technology side of what you are developing.\nCourse Logistics\nThe course will work in the following way.\nYou will have readings mostly due Tuesday of each week.\nWith some occasional exceptions for shorter readings that are due both Tues and Thurs, which will take place early in the semester to establish a few foundations (see schedule below).\nDuring the synchronous session on Tuesday, I will review the key highlights from your weekly reading and the Perusall questions/comments.\nThe rest of the class time on Tuesday and Thursday will be spent with me working individually with each team, and assisting with the project milestone for the week.\nI will break everyone into their team rooms where you can work together on the project milestone. I will have planned times to meet with each team during their breakout room session. Teams should be prepared with specific ideas/questions they would like to review where advice is needed to achieve the goals of the milestone\nWe will arrange a schedule for meeting in advance of the week. Each team will have a session alloted with me, and a session with Nawaaz. There is no necessity to meet with us (you can opt out), and it is intended as an aid to help unblock any challenges you may face along the process of building a data solution.\nOn select lecture dates we will have guest speakers\nThe topic we are studying is at the bleeding edge of applied practice in the data industry. As such it is useful to hear perspective on human centered design from leaders in industry. As their participation is confirmed it will be updated on the course schedule.\nCourse Schedule\nThe following schedule provides dates for readings. Most but not all readings will be assigned to Perusall. It has also marked the weeks where a project milestone begins and ends. The schedule will fill out with more detail as the semester progresses.\nWeek 01, 01/18 - 01/22\nThurs Jan 21\nTopic: Rstudio Global Conference\nRegister and attend any session(s) of the Rstudio global conference \nWeek 02, 01/25 - 01/29\nTues Jan 26\nTopic: Syllabus Day\nThurs Jan 28\nTopic: Introduction to Data Based Decisions\n📖 tfs (Introduction)  nudge (Chapter 1)\n\n📜 Bounthavong, M. (2019). Communicating data effectively with data visualizations: Part 21. URL: https://mbounthavong.com/blog/2019/12/13/communicating-data-effectively-with-data-visualizations-part-21-examples-of-famous-and-infamous-data-visualizations-1.\nSchneider, C., M. Weinmann, and J. Vom Brocke (2018). “Digital nudging: guiding online user choices through interface design”. In: Communications of the ACM 61.7, pp. 67-73. URL: https://cacm.acm.org/magazines/2018/7/229029-digital-nudging/fulltext#R25.\n\nWeek 03, 02/01 - 02/05\nProject: Start Problem Definition\nTues Feb 2\nTopic: Homo Economicus\n\n📜 Friedman, M. (1953). “The methodology of positive economics”. In: Essays in positive economics 3.3, pp. 145-178.\n\nThurs Feb 4\nTopic: R Markdown\n\n📜 Hohman, F., M. Conlen, J. Heer, et al. (2020). “Communicating with Interactive Articles”. In: Distill. https://distill.pub/2020/communicating-with-interactive-articles. DOI: 10.23915/distill.00028.\nWickham, H. (2017). “R Markdown”. In: R for Data Science. O’Reilly. Chap. 27.\n\nWeek 04, 02/08 - 02/12\nProject: Continue Problem Definition\nThursday Feb 11\nNo Class\nWeek 05, 02/15 - 02/19\nProject: Start User Persona\nWeek 06, 02/22 - 02/26\nProject: Continue User Persona\nWeek 07, 03/01 - 03/05\nProject: Start User Interviews\nWeek 08, 03/08 - 03/12\nProject: Continue User Interviews\nWeek 09, 03/15 - 03/19\nProject: Start Data Collection\nWeek 10, 03/22 - 03/26\nProject: Continue Data Collection\nWeek 11, 03/29 - 04/02\nProject: Build Prototype\nTues March 30\nNo class\nWeek 12, 04/05 - 04/09\nProject: Continue Build Prototype\nWeek 13, 04/12 - 04/16\nProject: Collect User Feedback\nWeek 14, 04/19 - 04/23\nProject: Continue Collect User Feedback\nWeek 15, 04/26 - 04/30\nProject: Iterate and Finalize\nWeek 16, 05/03 - 05/07\nProject: Continue Iterate and Finalize\n\n\n\nBecker, Gary S. 2017. Economic Theory. Routledge.\n\n\n\n\n",
    "preview": "https://db0ip7zd23b50.cloudfront.net/dims4/default/0dfba66/2147483647/legacy_thumbnail/960x369%3E/quality/90/?url=http%3A%2F%2Fbloomberg-bna-brightspot.s3.amazonaws.com%2F43%2F5e%2F9afb1c354b0e8ee71a13c737d4f5%2Fgettyimages-921135480.jpg",
    "last_modified": "2021-01-26T15:09:45+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-17-classsurvey/",
    "title": "First Day of Class",
    "description": "Rstudio Global Conference",
    "author": [
      {
        "name": "Amit K. Gandhi",
        "url": "upenn.edu"
      }
    ],
    "date": "2021-01-17",
    "categories": [],
    "contents": "\nThe Rstudio Global Conference\n\n\n\nAs I emailed you all last week, we will not have a synchronous class tomorrow but instead have asked you all to register and attend any sessions of interests at the Rstudio global conference taking place tomorrow The link to register is here.\nThere are many interesting talks taking place - you can see the full program here. I have earmarked a few talks that are personally interesting to me that I think resonate with topics/issues we will address in the course around using data to make decisions with technology. These are listed below and marked the ones in red that are specifically on the topic of data visualization, which is a problem we will discuss next week.\nI would pay particular attention to the opening keynote talk by Hadley Wickham (the chief scientist of Rstudio and designer of the Tidyverse framework for data science in R, which includes the famed visualization package ggplot).\n\n\n\n\n\ntime\n\n\npresenter\n\n\ntitle\n\n\nurl\n\n\n11:00AM\n\n\nHadley Wickham\n\n\nMaintaining the house the tidyverse built\n\n\nhttps://global.rstudio.com/student/page/40521\n\n\n01:00PM\n\n\nKara Woo\n\n\nAlways look on the bright side of plots\n\n\nhttps://global.rstudio.com/student/page/40618\n\n\n02:19PM\n\n\nMegan Beckett\n\n\nAesthetically automated figure production\n\n\nhttps://global.rstudio.com/student/page/40627\n\n\n01:19PM\n\n\nSean Lopp\n\n\nR & Python: Going Steady\n\n\nhttps://global.rstudio.com/student/page/40638\n\n\n01:19PM\n\n\nNicole Kramer\n\n\nA New Paradigm for Multifigure, Coordinate-Based Plotting in R\n\n\nhttps://global.rstudio.com/student/page/40633\n\n\n02:00PM\n\n\nSophie Beiers\n\n\nTrial and Error in Data Viz at the ACLU\n\n\nhttps://global.rstudio.com/student/page/40642\n\n\n03:00PM\n\n\nJohn Burn-Murdoch\n\n\nReporting on and visualising the pandemic\n\n\nhttps://global.rstudio.com/student/page/40615\n\n\n06:20PM\n\n\nRiva Quiroga\n\n\nHow to do things with words: learning to program in R with a “communicative approach”\n\n\nhttps://global.rstudio.com/student/page/40637\n\n\n05:00PM\n\n\nEmily Riederer\n\n\noRganization: How to make internal R packages part of your team\n\n\nhttps://global.rstudio.com/student/page/40607\n\n\n05:19PM\n\n\nMalcolm Barrett\n\n\nYou’re Already Ready: Zen and the Art of R Package Development\n\n\nhttps://global.rstudio.com/student/page/40621\n\n\n06:20PM\n\n\nAthanasia M. Mowinckel\n\n\nMake a package - Make some friends\n\n\nhttps://global.rstudio.com/student/page/40599\n\n\n06:25PM\n\n\nJohn Helveston\n\n\nUsing formr to create R-powered surveys with individualized feedback\n\n\nhttps://global.rstudio.com/student/page/40616\n\n\n06:35PM\n\n\nAlex Cookson\n\n\nThe Power of Great Datasets\n\n\nhttps://global.rstudio.com/student/page/40596\n\n\n05:00PM\n\n\nMax Kuhn\n\n\nWhat’s new in tidymodels?\n\n\nhttps://global.rstudio.com/student/page/40625\n\n\n05:16PM\n\n\nShirbi Ish-Shalom\n\n\nUsing R to Up Your Experimentation Game\n\n\nhttps://global.rstudio.com/student/page/40640\n\n\n06:23PM\n\n\nSimon Couch\n\n\ntidymodels/stacks, Or, In Preparation for Pesto: A Grammar for Stacked Ensemble Modeling\n\n\nhttps://global.rstudio.com/student/page/40641\n\n\n06:38PM\n\n\nAlan Feder\n\n\nCategorical Embeddings: New Ways to Simplify Complex Data\n\n\nhttps://global.rstudio.com/student/page/40595\n\n\n09:20PM\n\n\nEric Gunnar Cronstrom\n\n\nHow we made the switch: a case study on automating a complex report.\n\n\nhttps://global.rstudio.com/student/page/40608\n\n\n10:00PM\n\n\nRika\n\n\nFrom Zero to Hero: Best practices for setting up Rstudio Team in the Cloud\n\n\nhttps://global.rstudio.com/student/page/40636\n\n\n10:17PM\n\n\nDean Marchiori\n\n\nHow reproducible am I? A retrospective on a year of commercial data science projects in R\n\n\nhttps://global.rstudio.com/student/page/40606\n\n\n10:26PM\n\n\nCarson Sievert\n\n\nCustom theming in Shiny & R Markdown with bslib & thematic\n\n\nhttps://global.rstudio.com/student/page/40601\n\n\n09:18PM\n\n\nBarret Schloerke\n\n\nplumber + future: Async Web APIs\n\n\nhttps://global.rstudio.com/student/page/40600\n\n\n10:00PM\n\n\nNeal Richardson\n\n\nBigger Data With Ease Using Apache Arrow\n\n\nhttps://global.rstudio.com/student/page/40631\n\n\n10:19PM\n\n\nZJ\n\n\nEasy larger-than-RAM data manipulation with {disk.frame}\n\n\nhttps://global.rstudio.com/student/page/40647\n\n\n10:24PM\n\n\nGarrick Aden-Buie\n\n\nxaringan Playground: Using xaringan to learn web development\n\n\nhttps://global.rstudio.com/student/page/40609\n\n\n10:34PM\n\n\nLucy D’Agostino McGowan\n\n\nDesigning Randomized Studies using Shiny\n\n\nhttps://global.rstudio.com/student/page/40620\n\n\n\n\n\n",
    "preview": "https://rstudio.com/assets/img/rstudio-global-with-date.jpg",
    "last_modified": "2021-01-20T15:50:26+00:00",
    "input_file": {}
  }
]
