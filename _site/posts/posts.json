[
  {
    "path": "posts/2021-01-26-perusall-information/",
    "title": "Perusall Information",
    "description": "A short description of how Perusall works",
    "author": [
      {
        "name": "Amit K. Gandhi",
        "url": "upenn.edu"
      }
    ],
    "date": "2021-01-26",
    "categories": [],
    "contents": "\n\nHow Perusall Works\nPerusall helps you master readings faster, understand the material better, and get more out of your classes. To achieve this goal, you will be collaboratively annotating the textbook with others in your class.\nThe help you’ll get and provide your classmates (even if you don’t know anyone personally) will get you past confusions quickly and will make the process more fun. While you read, you’ll receive rapid answers to your questions, help others resolve their questions (which also helps you learn), and advise the instructor how to make class time most productive.\nYou can start a new annotation thread in Perusall by highlighting text, asking a question, or posting a comment; you can also add a reply or comment to an existing thread. Each thread is like a chat with one or more members of your class, and it happens in real time. Your goals in annotating each reading assignment are to stimulate discussion by posting good questions or comments and to help others by answering their questions.\n\n\n\n\n\n\n\n\n\n\n\nResearch shows that by annotating thoughtfully, you’ll learn more, so here’s what “annotating thoughtfully” means: Effective annotations deeply engage points in the readings, stimulate discussion, offer informative questions or comments, and help others by addressing their questions or confusions. To help you connect with classmates, you can “mention” a classmate in a comment or question to have them notified by email (they’ll also see a notification immediately if online), and you’ll also be notified when your classmates respond to your questions.\nFor each assignment, the application will evaluate the annotations you submit on time (see below). Based on the overall body of your annotations, you will receive a score for each assignment as follows\n\n3 = demonstrates exceptionally thoughtful and thorough reading of the entire assignment\n2 = demonstrates thoughtful and thorough reading of the entire assignment\n1 = demonstrates superficial reading of the entire assignment OR thoughtful reading of only part of the assignment\n0 = demonstrates superficial reading of only part of the assignment\n\nHow many annotations do I need to enter?\nWhen Perusall examines your annotations, its “assessment engine” has been designed to reflect the effort you put in your study of the text. It is unlikely that that effort will be reflected by just a few thoughtful annotations per assignment. On the other extreme, 30 per assignment is probably too many, unless a number of them are superficial or short comments or questions (which is fine, because it is OK to engage in chat with your peers). Somewhere in between these two extremes is about right and, thoughtful questions or comments that stimulate discussion or thoughtful and helpful answers to other students’ questions will earn you a higher score for the assignment.\nWhat does “on time” mean?\nThe work done in class depends on you having done the reading in advance. We will use the data arising from student responses on Perusall reading assignments as a guide for classroom discussion. Hence it is necessary to complete the reading and post your annotations before the deadline to receive credit. The deadline is a hard deadline, typically set 11pm on the night prior to the day of the lecture the reading is discussed.\n\n\n\n",
    "preview": "https://lms.unimelb.edu.au/__data/assets/image/0004/3355654/tile-perusall.png",
    "last_modified": "2021-01-26T15:09:59+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-25-course-introduction/",
    "title": "Course Introduction",
    "description": "Economics at the Intersection of Data and Technology",
    "author": [
      {
        "name": "Amit K. Gandhi",
        "url": "upenn.edu"
      }
    ],
    "date": "2021-01-25",
    "categories": [
      "Data",
      "A.I.",
      "Homo Economicus",
      "Behavioral Economics"
    ],
    "contents": "\n\nContents\nWelcome\nWhat is Market Design?\nWhat is the Topic of Our Class?Our approach to market design\nHow does this differ from data science?\n\nHow We Will LearnReading (50%)\nProject (50%)\n\nCourse Logistics\nCourse ScheduleWeek 01, 01/18 - 01/22\nWeek 02, 01/25 - 01/29\nWeek 03, 02/01 - 02/05\nWeek 04, 02/08 - 02/12\nWeek 05, 02/15 - 02/19\nWeek 06, 02/22 - 02/26\nWeek 07, 03/01 - 03/05\nWeek 08, 03/08 - 03/12\nWeek 09, 03/15 - 03/19\nWeek 10, 03/22 - 03/26\nWeek 11, 03/29 - 04/02\nWeek 12, 04/05 - 04/09\nWeek 13, 04/12 - 04/16\nWeek 14, 04/19 - 04/23\nWeek 15, 04/26 - 04/30\nWeek 16, 05/03 - 05/07\n\n\n\n\n\n \n \n Course \nEcon 262\n Professor\nAmit Gandhi (akgandhi@upenn.edu)\n TA\nNawaaz Khalfan (khalfan@sas.upenn.edu)\n Lecture \nTues, Thursday 1.30-2.50\n Office Hours \nBy appointment\n Website \nmarketdesign.io\n\n\nWelcome\nWelcome to Market Design (Econ 262)! This is a virtual course in the economics of decision making being taught in the Spring of 2021 at the University of Pennsylvania. This page describes the topic of the course, the learning objectives, the logistical plan, the evaluation policies, and the class schedule.\nThe course website will act as a central repository for the class to disseminate lecture materials and announcements. A GroupMe group for the class has been setup (reach out to Amit for access if you do not have it yet) and will serve as our platform to communicate with each other, conduct flash polls, and send reminders of class activities (including links for zoom class meetings.)\nWhat is Market Design?\nThe goal of market design is to design policies that improve the efficiency of economic decisions. An economic decision is any decision that involves trade-offs, which is synonymous with the very definition of economics as “the study of the allocation of scarce means to satisfy competing ends.” (Becker 2017)\nEconomic decisions are made by diverse actors in an economic system - households, firms, governments, and even the markets themselves (e.g., the invisible hand!).\nAn economic decision is efficient if it makes an optimal trade-off given the objectives and constraints of the decision maker.\nHelping make decisions more efficient creates value, generates profits, and raises welfare. But how should such interventions be designed? This is the central problem of market design.\nWhat is the Topic of Our Class?\nOur class is focused on the problems of market design in digital environments.\nHumans are increasingly making decisions in the presence of screens and devices, both at work and home. This has been especially pronounced during the last year of the pandemic where life quite literally moved online!\n\nA digital environment is one where computers and devices are networked together to facilitate human communication and transactions.\nWhat is the impact of digital environments on economic decisions? Does the transition from offline to digital environment improve economic efficiency? Can digital environments be designed to enable humans to better optimize their economic decisions?\nThese are central questions for market design in digital environments.\nA key difference between digital decisions and their offline counterparts is the role of data. A digital decision can leverage unprecedented velocity, variety, and volume  of data. The proliferation of data is a product of technological advancements in data science, data engineering, data analytics, machine learning, and A.I, which are distributed to decision makers through public cloud infrastructure.\nA fundamental question is then:\n\nMore Data = Better Decisions?\n\nThe answer is not as straightforward as it may seem. From a purely statistical or econometric viewpoint - more data is always better - we get better parameter inferences, better predictive accuracy, and better power to test models.\nHowever from a decision making perspective matters are less clear. The main problem is that data and A.I. - despite the hype - is not a panacea by itself. The technologies alone do not magically transform decisions. As economists are fond of saying, there is no free lunch!\nHal Varian (Chief Economist at Google and Professor at Berkeley) summarized the matter nicely in a recent interview:\n\nI think there’s a mystical belief in the power of data. Data is like oil in one respect… namely, it needs to be refined in order to be useful. So the data itself is not the important components, the know-how to refine it into something that’s useful. It’s the same [when] we talk about oil or data – it’s just the raw material, it’s not the finished product.\n\nIn order to create value, data technologies need to be transformed into applications that solve real problems and improve economic decisions. That is, applications need to be designed to extract value from data. Without good design, we are left with an alternative possibility:\n\nMore Data =  More Complexity =  Worse Decisions!\n\nA  central problem  for market design is  designing digital environment to translate “more data” into “better economic decisions”..\nOur approach to market design\nHow should digital environment be designed to enable efficient economic decisions?\nThe first rule of good design is to know your audience. For whom are we designing? Who is the user?\nLet me state the obvious answer which nevertheless has some profound implications for market design. The user is a human being!\n\nYou cannot understand good design if you do not understand people; design is made for people.\n\n— Dieter Rams\n\nOur approach in the class is to start with the user - the human - and understand how humans process and act upon data for economic decision making.\nHumans interact with raw data in a digital environment typically through a web application. The application processes, models, summarizes, and displays features of the raw data to the user, who internalizes the information in their behavior.\n\nThus from a user perspective, effective design in digital environments requires that a market designer to do 3 things:\n1. Identify an existing inefficiency in a human decision making process.\n2. Improve the the decision by leveraging data, technology, and economics.\n3. Influence humans to change their behavior toward the efficient outcome through digital interfaces.\n\nThe design problem encompasses all three steps. We can see that success in a market design project involves holistic thinking that integrates data/economic techniques with the visual elements of the digital interface, which are all tailored to the human behaviors we aim to effectuate and improve.\nThis is hard! It is a domain that is still in its infancy and being developed in real time across academic, business, and government as organizations digitally transform make data more central to their operation.\n\nHow does this differ from data science?\nA standard data science or machine learning classes will focus attention on the elements (2) (usually substituting “better decisions” with “better predictions”). These classes will focus on software techniques and algorithms for building and deploying ML algorithms.\nWe are instead interested in the underlying human decision problems that these algorithms and ultimately the data are intended to improve. Thus our focus is defining the problem (1) and generating influence (3), which provides the perspective necessary for solving (2).\nThus our focus is the human-centered design of data technologies to enhance economic decisions.\n\nI recognize that combining “human-centered” and “economic” together is not often how the “dismal science” is practiced or perceived. 😄\nHow We Will Learn\nIn order to solve the design problem above, we need to understand our user - human decision makers - in the wild. The investigation will proceed along two parallel tracks of learning in the class:\nReading\nCourse Project\nEach will count 50% towards your final course evaluation.\nThere is no curve in this class and hence no quotas on A’s or C’s. If you earnestly attempt to think and understand down each of these tracks, the metrics we have set up (and described below) will detect it and you will do well.\nReading (50%)\nA core learning experience in the course is rooted in reading the assigned book chapters and papers, and discussing them as a class. We will add a technological twist to this age old formula by adopting the  Perusall  application.\nOur Perusall code for the class is GANDHI-HUX9J. Go to to the Perusall website and enter this code to access our class library where all reading assignments will be posted.\nPerusall manages your engagement in the weekly readings. Perusall itself is based on an intelligent data technology that is aiding a key form of decision making in the classroom - student evaluations! Perusall measures engagement by your annotations in the reading, which forces you into an active reading mode through similar social forces that motivate our online media habits - it thus also embodies key features of Nudge (one of the books below)!\nOur readings fall into two categories:\nReading to gain perspective on human biases in decision making relative to efficient economic norms, and the effect of data on the manifestation of those biases.\nReading to learn tools for understanding human users and influencing them with the data scientific artifacts.\nWe describe the main texts that are part of the reading:\nThinking Fast and Slow\n\nIn order to identify and influence economic inefficiencies in the wild, we need a map to know where to look. If human departures from efficient decisions happen completely at random, then the search would be difficult if not impossible - we would be at the mercy of luck to successfully design!\nFortunately this is not the case. What is arguably the biggest discovery in the social and behavioral science in the last 50 years is that there are robust patterns of human cognition that make humans, e.g, Homo Sapien behave differently than the rational ideal, e.g., Homo Economicus.\nThe science we will apply to the question follows from the seminal work of Kahneman and Tversy (KT for short) who in 1971 began the first inquiry into the question of whether humans naturally are intuitive statisticians - e.g., do humans instinctively process data in a fashion that abides by formal statistical principles?\n\nThe surprising answer to the question was “no”, and led to an influential research paradigm known as “Heuristics and Biases” (H&B for short).\nThe crux of H&B, which is among the biggest discovery in the social and behavioral sciences of the last 50 years, is that there are robust patterns of human cognition that make humans, e.g, Homo Sapien behave differently than the rational ideal, e.g., Homo Economicus. The program spawned the field of behavioral economics and led to Kahneman being awarded the Nobel Prize for Economic Science in 2002 (Tversy sadly passed away in 1996).\nHomo Economicus is the \"rational actor&quot from your principles of economics courses used to model decisions by consumers, firms, and societies/governments.\nThe book “Thinking Fast and Slow” by Daniel Kahneman - describes the conceptual and historical backdrop behind H&B and provides the basic clues for where we can anticipate that sub-rational economic decisions will happen. Identifying these scenarios is a central part of being a market designer in the digital age - data and AI when properly employed should ameliorate these biases and improve economic efficiency.\nNudge (optional)\n\nIdentifying an inefficiency in an economic decision is one thing. Changing someone’s behavior towards a more efficient state is quite another. No one likes to be told what to do, especially from data they may not fully be aware or understand.\nPerhaps the answer lies not in directing the outcome of the decision, but rather to nudge the decision maker towards the rational outcome while respecting their autonomy over the decision.\nThis is the key idea behind the book Nudge by Richard Thaler and Cass Sunstein. Richard Thaler is a behavioral economist who developed the first economic applications of Kahneman and Tversky’s discoveries. He was awarded the Nobel prize himself in 2017.\nAn implication of H&B for market design is that it is not simply what data is presented to humans, but how they are presented that affects which economic decisions are made. It implies that both the presentation of information as well as the information itself are part of the design problem. Said another way - The interface matters!\nNudge applies this fact and recognizes that interfaces (what they call choice contexts) can manipulated and hence designed to produce decisions that are more rational. Thaler and Sunstein call this type of market design choice architecture - designing the choice environment to nudge people to behave more rationally without having to make people “think” more rationally!\nHowever the practice of designing nudges has a potential dark side - especially in digital environments. They can be used to exploit human frailties to extract profits in a way that may not be human welfare improving to the user. Of course the definition of welfare itself can be in the eye of the beholder, and the dividing line between good vs evil nudges is not always clear. We will aim to be mindful of this important complexity as we explore the topic.\nR for Data Science\n\nBoth “Thinking Fast and Slow” and “Nudge” were written before data science came of age. In fact the term “data science” itself was coined in 2008 (by D.J. Patil and Jeff Hammerbacher), the same year as Nudge was published.\n\nData science is in many ways represents the modern day choice architecture. Data scientist in organizations across industries are processing, filtering, and designing displays of information that defines the context for a large variety of our economic decisions.\nData science should be seen as both a set of tools for working with data, as well as a process for answering questions and solving problems with data to influence stakeholders seeking value from data (typically decision makers who are the clients or customers of a data analysis). The book R for Data Science written by Hadley Wickham has become a ``modern classic’’ in establishing an elegant and powerful set of tools and processes.\nThe tools he describes, including ggplot, dplyr, and tibbles, are the staples of being a productive data scientist. The process, which sometimes receives less attention, is just as powerful and encapsulated by the following data science workflow: \nThe tidyverse is a collection of packages that have a common set of design principles and interfaces that juxtaposes against this workflow to create a rich ecosystem for data science. A sampling of the most popular packages can be seen here: \nParticularly of interest for digital design of decisions is the communication step. As can be seen, communication is the last mile of the data scientific workflow and it is arguably the most important (and unfortunately least taught!). The communicaton step is associated to a specific tooling element: rmarkdown.\nRmarkdown weaves together code, text, and visualizations in a single software medium intended for human consumption. It lives as a text file, e.g., a piece of code, which can be versioned and generates a communication that is reproducible. We will examine rmarkdown and the process of building data based communications that can influence the behavior of human decision makers.\nProject (50%)\nYou will work in teams of 3-4 and engage in a human centered design process that applies data to help solve a decision problem. The process will entail 7 distinct stages that you will carry your project forward with your team. The stages correspond to the elements of a human centered design process for data solutions:\n\nProject Flow\n\n\n\n{\"x\":{\"diagram\":\"digraph{\\n\\n                     graph[rankdir = TD]\\n                     \\n                     node[shape = rectangle, style = filled, fontsize = 44]  \\n                     A[label = \\\"Decision Problem\\\"]\\n                     B[label = \\\"User Persona\\\"]\\n                     C[label = \\\"User Interview\\\"]\\n                     D[label = \\\"Data Collection\\\"]\\n                     E[label = \\\"Construct Prototype\\\"]\\n                     F[label = \\\"Collect Feedback\\\"]\\n                     G[label = \\\"Iterate and Finalize\\\"]\\n\\n                     edge[color = black, fontsize = 44]\\n                     A -> B\\n                     B -> C\\n                     C -> D\\n                     D -> E\\n                     E -> F\\n                     F -> G\\n                     \\n                     }\",\"config\":{\"engine\":\"dot\",\"options\":null}},\"evals\":[],\"jsHooks\":[]}\nThe components are:\n1: Define a decision problem of interest/concern 2: Hypothesize user personas and recruit users for interviews 3: Conduct user interviews and test/iterate the problem and persona definitions 4: Collect data for the problem 5: Construct a prototype 6: Collect user feedback on the prototype 7: Iterate and finalize prototype \nYour journey along this path will documented in a team blog. Each stage is a milestone in the project. Your output from each stage will correspond to a blog post. We will use rmarkdown for creating team blogs. Instructions for building your blog with Rmarkdown will be provided in the class.\nThe goal of the project is to get you to experience first hand what I view as the most challenging aspect of the design problem. This is usually not the technical AI, ML, or Data Science component, but rather understanding the user experience and user value for the decision being improved. The particular data technique that factors into building a prototype can be very simple - a well designed data visualization for example can more than suffice for a successful prototype, but it is the discretion of the team to find the appropriate data design for the problem at hand.\nYou are not being evaluated on the intricacy of the prototype- there is no failure! Your evaluation is based on attempting to follow the process to develop a useful scenario for the application of data to the decision problem you have scoped. A big part of success is thus having a well defined problem, which itself will feel ambiguous and will morph as you go along the process.\nThus you should see the process as an education in itself in real world design of data solution. You will struggle with ambiguity and test ideas, and discard many false starts before you find a path towards a solution that adds value to a decision problem. This is the nature of the iterative method and keeps you constantly connected to the user scenario before investing heavily in the technology side of what you are developing.\nCourse Logistics\nThe course will work in the following way.\nYou will have readings mostly due Tuesday of each week.\nWith some occasional exceptions for shorter readings that are due both Tues and Thurs, which will take place early in the semester to establish a few foundations (see schedule below).\nDuring the synchronous session on Tuesday, I will review the key highlights from your weekly reading and the Perusall questions/comments.\nThe rest of the class time on Tuesday and Thursday will be spent with me working individually with each team, and assisting with the project milestone for the week.\nI will break everyone into their team rooms where you can work together on the project milestone. I will have planned times to meet with each team during their breakout room session. Teams should be prepared with specific ideas/questions they would like to review where advice is needed to achieve the goals of the milestone\nWe will arrange a schedule for meeting in advance of the week. Each team will have a session alloted with me, and a session with Nawaaz. There is no necessity to meet with us (you can opt out), and it is intended as an aid to help unblock any challenges you may face along the process of building a data solution.\nOn select lecture dates we will have guest speakers\nThe topic we are studying is at the bleeding edge of applied practice in the data industry. As such it is useful to hear perspective on human centered design from leaders in industry. As their participation is confirmed it will be updated on the course schedule.\nCourse Schedule\nThe following schedule provides dates for readings. Most but not all readings will be assigned to Perusall. It has also marked the weeks where a project milestone begins and ends. The schedule will fill out with more detail as the semester progresses.\nWeek 01, 01/18 - 01/22\nThurs Jan 21\nTopic: Rstudio Global Conference\nRegister and attend any session(s) of the Rstudio global conference \nWeek 02, 01/25 - 01/29\nTues Jan 26\nTopic: Syllabus Day\nThurs Jan 28\nTopic: Introduction to Data Based Decisions\n📖 tfs (Introduction)  nudge (Chapter 1)\n\n📜 Bounthavong, M. (2019). Communicating data effectively with data visualizations: Part 21. URL: https://mbounthavong.com/blog/2019/12/13/communicating-data-effectively-with-data-visualizations-part-21-examples-of-famous-and-infamous-data-visualizations-1.\nSchneider, C., M. Weinmann, and J. Vom Brocke (2018). “Digital nudging: guiding online user choices through interface design”. In: Communications of the ACM 61.7, pp. 67-73. URL: https://cacm.acm.org/magazines/2018/7/229029-digital-nudging/fulltext#R25.\n\nWeek 03, 02/01 - 02/05\nProject: Start Problem Definition\nTues Feb 2\nTopic: Homo Economicus\n\n📜 Friedman, M. (1953). “The methodology of positive economics”. In: Essays in positive economics 3.3, pp. 145-178.\n\nThurs Feb 4\nTopic: R Markdown\n\n📜 Hohman, F., M. Conlen, J. Heer, et al. (2020). “Communicating with Interactive Articles”. In: Distill. https://distill.pub/2020/communicating-with-interactive-articles. DOI: 10.23915/distill.00028.\nWickham, H. (2017). “R Markdown”. In: R for Data Science. O’Reilly. Chap. 27.\n\nWeek 04, 02/08 - 02/12\nProject: Continue Problem Definition\nThursday Feb 11\nNo Class\nWeek 05, 02/15 - 02/19\nProject: Start User Persona\nWeek 06, 02/22 - 02/26\nProject: Continue User Persona\nWeek 07, 03/01 - 03/05\nProject: Start User Interviews\nWeek 08, 03/08 - 03/12\nProject: Continue User Interviews\nWeek 09, 03/15 - 03/19\nProject: Start Data Collection\nWeek 10, 03/22 - 03/26\nProject: Continue Data Collection\nWeek 11, 03/29 - 04/02\nProject: Build Prototype\nTues March 30\nNo class\nWeek 12, 04/05 - 04/09\nProject: Continue Build Prototype\nWeek 13, 04/12 - 04/16\nProject: Collect User Feedback\nWeek 14, 04/19 - 04/23\nProject: Continue Collect User Feedback\nWeek 15, 04/26 - 04/30\nProject: Iterate and Finalize\nWeek 16, 05/03 - 05/07\nProject: Continue Iterate and Finalize\n\n\n\nBecker, Gary S. 2017. Economic Theory. Routledge.\n\n\n\n\n",
    "preview": "https://db0ip7zd23b50.cloudfront.net/dims4/default/0dfba66/2147483647/legacy_thumbnail/960x369%3E/quality/90/?url=http%3A%2F%2Fbloomberg-bna-brightspot.s3.amazonaws.com%2F43%2F5e%2F9afb1c354b0e8ee71a13c737d4f5%2Fgettyimages-921135480.jpg",
    "last_modified": "2021-01-26T15:09:45+00:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-01-17-classsurvey/",
    "title": "First Day of Class",
    "description": "Rstudio Global Conference",
    "author": [
      {
        "name": "Amit K. Gandhi",
        "url": "upenn.edu"
      }
    ],
    "date": "2021-01-17",
    "categories": [],
    "contents": "\nThe Rstudio Global Conference\n\n\n\nAs I emailed you all last week, we will not have a synchronous class tomorrow but instead have asked you all to register and attend any sessions of interests at the Rstudio global conference taking place tomorrow The link to register is here.\nThere are many interesting talks taking place - you can see the full program here. I have earmarked a few talks that are personally interesting to me that I think resonate with topics/issues we will address in the course around using data to make decisions with technology. These are listed below and marked the ones in red that are specifically on the topic of data visualization, which is a problem we will discuss next week.\nI would pay particular attention to the opening keynote talk by Hadley Wickham (the chief scientist of Rstudio and designer of the Tidyverse framework for data science in R, which includes the famed visualization package ggplot).\n\n\n\n\n\ntime\n\n\npresenter\n\n\ntitle\n\n\nurl\n\n\n11:00AM\n\n\nHadley Wickham\n\n\nMaintaining the house the tidyverse built\n\n\nhttps://global.rstudio.com/student/page/40521\n\n\n01:00PM\n\n\nKara Woo\n\n\nAlways look on the bright side of plots\n\n\nhttps://global.rstudio.com/student/page/40618\n\n\n02:19PM\n\n\nMegan Beckett\n\n\nAesthetically automated figure production\n\n\nhttps://global.rstudio.com/student/page/40627\n\n\n01:19PM\n\n\nSean Lopp\n\n\nR & Python: Going Steady\n\n\nhttps://global.rstudio.com/student/page/40638\n\n\n01:19PM\n\n\nNicole Kramer\n\n\nA New Paradigm for Multifigure, Coordinate-Based Plotting in R\n\n\nhttps://global.rstudio.com/student/page/40633\n\n\n02:00PM\n\n\nSophie Beiers\n\n\nTrial and Error in Data Viz at the ACLU\n\n\nhttps://global.rstudio.com/student/page/40642\n\n\n03:00PM\n\n\nJohn Burn-Murdoch\n\n\nReporting on and visualising the pandemic\n\n\nhttps://global.rstudio.com/student/page/40615\n\n\n06:20PM\n\n\nRiva Quiroga\n\n\nHow to do things with words: learning to program in R with a “communicative approach”\n\n\nhttps://global.rstudio.com/student/page/40637\n\n\n05:00PM\n\n\nEmily Riederer\n\n\noRganization: How to make internal R packages part of your team\n\n\nhttps://global.rstudio.com/student/page/40607\n\n\n05:19PM\n\n\nMalcolm Barrett\n\n\nYou’re Already Ready: Zen and the Art of R Package Development\n\n\nhttps://global.rstudio.com/student/page/40621\n\n\n06:20PM\n\n\nAthanasia M. Mowinckel\n\n\nMake a package - Make some friends\n\n\nhttps://global.rstudio.com/student/page/40599\n\n\n06:25PM\n\n\nJohn Helveston\n\n\nUsing formr to create R-powered surveys with individualized feedback\n\n\nhttps://global.rstudio.com/student/page/40616\n\n\n06:35PM\n\n\nAlex Cookson\n\n\nThe Power of Great Datasets\n\n\nhttps://global.rstudio.com/student/page/40596\n\n\n05:00PM\n\n\nMax Kuhn\n\n\nWhat’s new in tidymodels?\n\n\nhttps://global.rstudio.com/student/page/40625\n\n\n05:16PM\n\n\nShirbi Ish-Shalom\n\n\nUsing R to Up Your Experimentation Game\n\n\nhttps://global.rstudio.com/student/page/40640\n\n\n06:23PM\n\n\nSimon Couch\n\n\ntidymodels/stacks, Or, In Preparation for Pesto: A Grammar for Stacked Ensemble Modeling\n\n\nhttps://global.rstudio.com/student/page/40641\n\n\n06:38PM\n\n\nAlan Feder\n\n\nCategorical Embeddings: New Ways to Simplify Complex Data\n\n\nhttps://global.rstudio.com/student/page/40595\n\n\n09:20PM\n\n\nEric Gunnar Cronstrom\n\n\nHow we made the switch: a case study on automating a complex report.\n\n\nhttps://global.rstudio.com/student/page/40608\n\n\n10:00PM\n\n\nRika\n\n\nFrom Zero to Hero: Best practices for setting up Rstudio Team in the Cloud\n\n\nhttps://global.rstudio.com/student/page/40636\n\n\n10:17PM\n\n\nDean Marchiori\n\n\nHow reproducible am I? A retrospective on a year of commercial data science projects in R\n\n\nhttps://global.rstudio.com/student/page/40606\n\n\n10:26PM\n\n\nCarson Sievert\n\n\nCustom theming in Shiny & R Markdown with bslib & thematic\n\n\nhttps://global.rstudio.com/student/page/40601\n\n\n09:18PM\n\n\nBarret Schloerke\n\n\nplumber + future: Async Web APIs\n\n\nhttps://global.rstudio.com/student/page/40600\n\n\n10:00PM\n\n\nNeal Richardson\n\n\nBigger Data With Ease Using Apache Arrow\n\n\nhttps://global.rstudio.com/student/page/40631\n\n\n10:19PM\n\n\nZJ\n\n\nEasy larger-than-RAM data manipulation with {disk.frame}\n\n\nhttps://global.rstudio.com/student/page/40647\n\n\n10:24PM\n\n\nGarrick Aden-Buie\n\n\nxaringan Playground: Using xaringan to learn web development\n\n\nhttps://global.rstudio.com/student/page/40609\n\n\n10:34PM\n\n\nLucy D’Agostino McGowan\n\n\nDesigning Randomized Studies using Shiny\n\n\nhttps://global.rstudio.com/student/page/40620\n\n\n\n\n\n",
    "preview": "https://rstudio.com/assets/img/rstudio-global-with-date.jpg",
    "last_modified": "2021-01-20T15:50:26+00:00",
    "input_file": {}
  }
]
