---
title: "Course Introduction"
description: |
  Economics at the Intersection of Data and Technology
author:
  - name: Amit K. Gandhi
    url: upenn.edu
date: 1-25-2021
output:
  distill::distill_article:
    self_contained: false
    toc: true
    toc_depth: 2
bibliography: ../../biblio.json
categories:
  - Data 
  - A.I.
  - Homo Economicus
  - Behavioral Economics
preview: https://db0ip7zd23b50.cloudfront.net/dims4/default/0dfba66/2147483647/legacy_thumbnail/960x369%3E/quality/90/?url=http%3A%2F%2Fbloomberg-bna-brightspot.s3.amazonaws.com%2F43%2F5e%2F9afb1c354b0e8ee71a13c737d4f5%2Fgettyimages-921135480.jpg
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(DiagrammeR)

colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}

library(RefManageR)
bib <- ReadBib("../../biblio.bib")
#myopts <- BibOptions(cite.style = "authoryear", bib.style = "authoryear", style="markdown", first.inits=FALSE, max.names = 20)

BibOptions(check.entries = FALSE, style = "markdown", bib.style = "authoryear", cite.style = 'authoryear')

mon <- as.Date("2021-01-18")

advdate <- function(obj, adv) {
 tmon <- obj + 7*(adv-1)
 tfri <- obj + 4 + 7*(adv-1)
 tmon <- format(tmon, format="%m/%d")
 tfri <- format(tfri, format="%m/%d")
 zadv <- sprintf("%02d", adv)
 tmp <- paste("Week ",zadv,sep='',", ", tmon," - ",tfri)
 return(tmp)
}
```

![](https://db0ip7zd23b50.cloudfront.net/dims4/default/0dfba66/2147483647/legacy_thumbnail/960x369%3E/quality/90/?url=http%3A%2F%2Fbloomberg-bna-brightspot.s3.amazonaws.com%2F43%2F5e%2F9afb1c354b0e8ee71a13c737d4f5%2Fgettyimages-921135480.jpg)

<font size = "12pt">

<center>

&nbsp; | &nbsp;
  --- | ---
| <font color = "red"> Course </font> | Econ 262
| <font color = "red">   Professor | Amit Gandhi (<akgandhi@upenn.edu>)
| <font color = "red"> TA| Nawaaz Khalfan (<khalfan@sas.upenn.edu>)
| <font color = "red"> Lecture </font> | Tues, Thursday 1.30-2.50
| <font color = "red"> Office Hours </font> | By appointment
| <font color = "red"> Website </font> | [marketdesign.io](marketdesign.io)

</center>
</font>


# Welcome {.salt}

**Welcome to Market Design (Econ 262)**! This is a virtual course in the **economics of decision making** being taught in the Spring of 2021 at the **University of Pennsylvania**. This page describes the topic of the course, the learning objectives, the logistical plan, the evaluation policies, and the class schedule. 


The course [website](marketdesign.io) will act as a central repository for the class to disseminate lecture materials and announcements. A **GroupMe** group for the class has been setup (reach out to Amit for access if you do not have it yet) and will serve as our platform to communicate with each other, conduct flash polls, and send reminders of class activities (including links for zoom class meetings.)


# What is Market Design?{.salt}

The goal of market design is to design policies that improve the efficiency of economic decisions. An **economic decision** is any decision that involves trade-offs, which is synonymous with the very definition of economics as “the study of the allocation of scarce means to satisfy competing ends.” [@becker2017economic]

Economic decisions are made by diverse actors in an economic system - **households**, **firms**, **governments**, and even the **markets** themselves (e.g., the invisible hand!). 

An economic decision is **efficient** if it makes an optimal trade-off given the objectives and constraints of the decision maker.

Helping make decisions more efficient **creates value, generates profits, and raises welfare**. But how should such interventions be designed? This is the central problem of market design. 

# What is the Topic of Our Class?{.salt}
Our class is focused on the problems of market design in **digital environments**.

Humans are increasingly making decisions in the presence of **screens** and **devices**, both at work and home. This has been especially pronounced during the last year of the pandemic where life quite literally moved online! 

<aside>
A digital environment is one where computers and devices are networked together to facilitate human communication and transactions. 
</aside>


**What is the impact of digital environments on economic decisions?** Does the transition from offline to digital environment improve economic efficiency? Can digital environments be **designed** to enable humans to better optimize their economic decisions? 

These are central questions for market design in digital environments.  

A key difference between digital decisions and their offline counterparts is the role of **data**. A digital decision can leverage unprecedented  **velocity, variety, and volume ** of data. The proliferation of data is a product of technological advancements in *data science, data engineering, data analytics, machine learning, and A.I*, which are distributed to decision makers through  **public cloud infrastructure**. 

A fundamental question is then:

<center>

### More Data = Better Decisions? {.heat}

</center>

The answer is not as straightforward as it may seem. From a purely statistical or econometric viewpoint - more data is always better - we get better parameter inferences, better predictive accuracy, and better power to test models. 

However from a decision making perspective matters are less clear. The main problem is that data and A.I. - despite the hype - is not a panacea by itself.  **The technologies alone do not magically transform decisions.** As economists are fond of saying, **there is no free lunch**! 

Hal Varian (Chief Economist at Google and Professor at Berkeley) summarized the matter nicely in a recent interview:

> I think there’s a mystical belief in the power of data. Data is like oil in one respect… namely, it needs to be refined in order to be useful. So the data itself is not the important components, the know-how to refine it into something that’s useful. It’s the same [when] we talk about oil or data – it’s just the raw material, it’s not the finished product.

In order to create value,  data technologies need to be transformed into applications that solve real problems and improve economic decisions. That is, applications need to be **designed** to extract value from data. Without good design, we are left with an alternative possibility:

<center>

### More Data = <br> More Complexity = <br> Worse Decisions! {.heat} 

</center>


A <font size = 4 color = "red"> central problem </font> for market design  is <font size = 4 color = "red"> designing digital environment to translate "more data" into "better economic decisions".</font>. 


## Our approach to market design

How should digital environment be designed to enable efficient economic decisions?

**The first rule of good design is to know your audience.** For whom are we designing? Who is the user? 

Let me state the obvious answer which nevertheless has some profound implications for market design. **The user is a human being**!


>You cannot understand good design if you do not understand people; design is made for people.
>`r tufte::quote_footer('--- Dieter Rams')`

Our approach in the class is to start with the user - the human - and understand how humans process and act upon data for economic decision making. 

Humans interact with raw data in a digital environment typically through a web application. The application processes, models, summarizes, and displays features of the raw data to the user, who internalizes the information in their behavior. 

![](https://boostlabs.com/wp-content/uploads/2019/09/10-types-of-data-visualization-1.jpg)

Thus from a user perspective, effective design in digital environments requires that a market designer to do 3 things:

**1. Identify** an existing inefficiency in a human decision making process.

**2. Improve** the the decision by leveraging data, technology, and economics.

**3. Influence** humans to change their behavior toward the efficient outcome through digital interfaces.

<!-- This is where it differs from consulting - you are building a product, and the 3 steps progress in an iterative fashion. You are in college to learn the "Deep thinking" that is (2) but that is also inefficient.-->


The design problem encompasses all three steps. We can see that success in a market design project involves holistic thinking that integrates data/economic techniques  with the visual elements of the digital interface, which are all tailored to the human behaviors we aim to effectuate and improve. 

This is hard! It is a domain that is still in its infancy and being developed in real time across academic, business, and government as organizations digitally transform make data more central to their operation. 

<!-- It is thus a course that (by design `r emo::ji("smile")`) will aim to pose many more questions than necessarily answering them.  -->

## How does this differ from data science? 

A standard data science or machine learning classes will focus attention on the elements **(2)** (usually substituting "better decisions" with "better predictions"). These classes will focus on software techniques and algorithms for building and deploying ML algorithms. 

We are instead interested in the underlying human decision problems that these algorithms and ultimately the data are intended to improve. Thus our focus is defining the problem **(1)** and generating influence **(3)**, which provides the perspective necessary for solving **(2)**.  

Thus our focus is the *human-centered design of data technologies to enhance economic decisions*.

<aside>
I recognize that combining "human-centered" and "economic" together is not often how the "dismal science" is practiced or perceived. `r emo::ji("smile")`
</aside> 

# How We Will Learn {.salt}

In order to solve the design problem above, we need to understand our user - *human decision makers* - **in the wild**. The investigation will proceed along two parallel tracks of learning in the class:

1. Reading 
2. Course Project 

Each will count 50% towards your final course evaluation.

There is no curve in this class and hence no quotas on A's or C's. If you earnestly attempt to think and understand down each of these tracks, the metrics we have set up (and described below) will detect it and you will do well.

## Reading (50%)

A core learning experience in the course is rooted in reading the assigned book chapters and papers, and discussing them as a class. We will add a technological twist to this age old formula by adopting the  <font size = 4 color = "red"> Perusall </font> application. 

Our Perusall code for the class is GANDHI-HUX9J. Go to ![the Perusall website](www.perusall.com) and enter this code to access our class library where all reading assignments will be posted. 

Perusall manages your engagement in the weekly readings. Perusall itself is based on an intelligent data technology that is aiding a key form of decision making in the classroom - student evaluations! Perusall measures engagement by your annotations in the reading, which forces you into an active reading mode through similar social forces that motivate our online media habits - it thus also embodies key features of Nudge (one of the books below)!

Our readings fall into two categories:

1. Reading to gain perspective on human biases in decision making relative to efficient economic norms, and the effect of data on the manifestation of those biases. 
2. Reading to learn tools for understanding human users and influencing them with the data scientific artifacts.

We describe the main texts that are part of the reading:

### Thinking Fast and Slow{.heat}

![](https://www.kurzweilai.net/images/book-cover-Thinking-Fast-+-Slow-no.-5.png)

In order to identify and influence economic inefficiencies in the wild, we need a map to know where to look. If human departures from efficient decisions happen completely at random, then the search would be difficult if not impossible - we would be at the mercy of luck to successfully design! 

Fortunately this is not the case. What is arguably the biggest discovery in the social and behavioral science in the last 50 years is that there are robust patterns of human cognition that make humans, e.g, **Homo Sapien** behave differently than the rational ideal, e.g., **Homo Economicus**.


The science we will apply to the question follows from the seminal work of Kahneman and Tversy (KT for short) who in 1971 began the first inquiry into the question of whether humans naturally are **intuitive statisticians** - e.g., do humans instinctively process data in a fashion that abides by formal statistical principles?

![](https://media.vanityfair.com/photos/58262be9364154776b0baa17/1:1/w_1016,h_1016,c_limit/michael-lewis-moneyball-daniel-kahneman-amos-tversky-01.jpg) ![](images/lawofsmallnumbers.png)

The surprising answer to the question was "no", and led to an influential research paradigm known as **"Heuristics and Biases"** (H&B for short). 

The crux of H&B, which is among the biggest discovery in the social and behavioral sciences of the last 50 years,  is that there are robust patterns of human cognition that make humans, e.g, **Homo Sapien** behave differently than the rational ideal, e.g., **Homo Economicus**.  The program spawned the field of behavioral economics and led to Kahneman being awarded the **Nobel Prize for Economic Science** in 2002 (Tversy sadly passed away in 1996). 

<aside>
**Homo Economicus** is the &quot;rational actor&quot from your principles of economics courses used to model decisions by consumers, firms, and societies/governments.
</aside>


The book  **"Thinking Fast and Slow"** by *Daniel Kahneman* - describes the conceptual and historical backdrop behind H&B and provides the basic clues for where we can anticipate that sub-rational economic decisions will happen. Identifying these scenarios is a central part of being a market designer in the digital age - data and AI when properly employed should ameliorate these biases and improve economic efficiency.

### Nudge (optional) {.heat}

![](https://cms.qz.com/wp-content/uploads/2018/10/Nudge.jpg?quality=75&strip=all&w=1600&h=900&crop=1g)

Identifying an inefficiency in an economic decision is one thing. Changing someone's behavior towards a more efficient state is quite another. No one likes to be told what to do, especially from data they may not fully be aware or understand. 

Perhaps the answer lies not in directing the outcome of the decision, but rather to *nudge* the decision maker towards the rational outcome while respecting their autonomy over the decision. 

This is the key idea behind the book **Nudge** by Richard Thaler and Cass Sunstein. Richard Thaler is a behavioral economist who developed the first economic applications of Kahneman and Tversky's discoveries. He was awarded the Nobel prize himself in 2017. 

An implication of H&B for market design is that it is not simply *what* data is presented to humans, but **how** they are presented that affects which economic decisions are made. It implies that both the presentation of information as well as the information itself are part of the design problem. Said another way - **The interface matters!**

Nudge applies this fact and recognizes that interfaces (what they call **choice contexts**) can manipulated and hence designed to produce decisions that are more rational. Thaler and Sunstein call this type of market design *choice architecture* - designing the choice environment to nudge people to behave more rationally without having to make  people "think" more rationally! 

However the practice of designing nudges has a potential **dark side** - especially in digital environments. They can be used to exploit human frailties to extract profits in a way that may not be human welfare improving to the user. Of course the definition of *welfare* itself can be in the eye of the beholder, and the dividing line between *good* vs *evil* nudges is not always clear. We will aim to be mindful of this important complexity as we explore the topic. 

### R for Data Science{.heat}
![](https://d33wubrfki0l68.cloudfront.net/bd366e1a176f391d0e6f092439ee92a0e99875c6/6250e/wp-content/uploads/2018/08/r-for-data-science.jpg)

Both "Thinking Fast and Slow" and "Nudge" were written before data science came of age. In fact the term "data science" itself was coined in 2008 (by D.J. Patil and Jeff Hammerbacher), the same year as *Nudge* was published.

<!-- Furthermore, the audience that "Nudge" is targeting is an audience of policy makers in positions of power and authority. This does not describe the lives of many young people entering the workforce who will be increasingly working directly with data to solve problems.  -->

Data science is in many ways represents the modern day choice architecture. Data scientist in organizations across industries are processing, filtering, and designing displays of information that defines the context for a large variety of our economic decisions.

Data science should be seen as both a set of **tools** for working with data, as well as a **process** for answering questions and solving problems with data to influence stakeholders seeking value from data (typically decision makers who are the clients or customers of a data analysis). The book **R for Data Science** written by Hadley Wickham has become a ``modern classic'' in establishing an elegant and powerful set of tools and processes. 

The tools he describes, including *ggplot*, *dplyr*, and *tibbles*, are the staples of being a productive data scientist. The process, which sometimes receives less attention, is just as powerful and encapsulated by the following data science workflow:
![](https://d33wubrfki0l68.cloudfront.net/571b056757d68e6df81a3e3853f54d3c76ad6efc/32d37/diagrams/data-science.png)

The `tidyverse` is a collection of packages that have a common set of design principles and interfaces that juxtaposes against this workflow to create a rich ecosystem for data science. A sampling of the most popular packages can be seen here:
![](https://miro.medium.com/max/4032/1*B-cwhqnFgGIbd9lWnzi_mQ.png)

Particularly of interest for digital design of decisions is the **communication** step. As can be seen, communication is the last mile of the data scientific workflow and it is arguably the most important (and unfortunately least taught!). The communicaton step is associated to a specific tooling element: **rmarkdown**. 

Rmarkdown weaves together code, text, and visualizations in a single software medium intended for human consumption. It lives as a text file, e.g., a piece of code, which can be versioned and generates a communication that is reproducible. We will examine rmarkdown and the process of building data based communications that can influence the behavior of human decision makers. 

## Project (50%)


You will work in teams of 3-4 and engage in a human centered design process that applies data to help solve a decision problem. The process will entail 7 distinct stages that you will carry your project forward with your team. The stages correspond to the elements of a human centered design process for data solutions:

<center>
### Project Flow {.heat}
</center>

```{r, fig.width=10, fig.height=6}
my_graphviz <- grViz("digraph{

                     graph[rankdir = TD]
                     
                     node[shape = rectangle, style = filled, fontsize = 44]  
                     A[label = 'Decision Problem']
                     B[label = 'User Persona']
                     C[label = 'User Interview']
                     D[label = 'Data Collection']
                     E[label = 'Construct Prototype']
                     F[label = 'Collect Feedback']
                     G[label = 'Iterate and Finalize']

                     edge[color = black, fontsize = 44]
                     A -> B
                     B -> C
                     C -> D
                     D -> E
                     E -> F
                     F -> G
                     
                     }")

my_graphviz
```

The components are:



**1**: *Define a decision problem of interest/concern* <br>
**2**: *Hypothesize user personas and recruit users for interviews* <br>
**3**: *Conduct user interviews and test/iterate the problem and persona definitions* <br>
**4**: *Collect data for the problem* <br>
**5**: *Construct a prototype* <br>
**6**: *Collect user feedback on the prototype* <br>
**7**: *Iterate and finalize prototype* <br>



Your journey along this path will documented in a **team blog**. Each stage is a milestone in the project. Your output from each stage will correspond to a **blog post**. We will use **rmarkdown** for creating team blogs. Instructions for building your blog with Rmarkdown will be provided in the class. 

The goal of the project is to get you to experience first hand the challenging aspect of the **design problem**. This is usually not the technical AI, ML, or Data Science component, but rather **understanding the user experience** and user value for the decision being improved. The particular data technique that factors into building a prototype can be very simple - a well designed **data visualization** for example can more than suffice for a successful prototype, but it is the discretion of the team to find the appropriate data design for the problem at hand. 

You are not being evaluated on the intricacy of the prototype- **there is no failure**! Your evaluation is based on attempting to follow the process to develop a useful scenario for the application of data to the decision problem you have scoped. A big part of success is thus having a **well defined problem**, which itself will feel ambiguous and will morph as you go along the process. 

Thus you should see the process as an education in itself in real world design of data solution. You will **struggle with ambiguity** and test ideas, and discard many false starts before you find a path towards a solution that adds value to a decision problem. This is the nature of the **iterative method** and keeps you constantly connected to the **user scenario** before investing heavily in the technology side of what you are developing. 

# Course Logistics {.salt}

The course will work in the following way. 

### You will have readings mostly due Tuesday of each week
With some occasional exceptions on readings due Tues and Thurs early in the semester to establish a few foundations

### During the synchronous session on Tuesday I will review main highlights from your weekly reading and the Perusall questions/comments

### The rest of the class time on Tuesday and Thursday will be spent with me working individually with teams on assisting with the project milestone during for the week.

Teams should be prepared with specific ideas/questions they would like to review where advice is needed to achieve the goals of the milestone

We will arrange a schedule of working with the teams for each week. There will be time allotted for me to work with each team once a week and Nawaaz to work one once each week. There is no necessity to meet with us (you can opt out), and it is intended as an aid to unblock you on challenges you may face with your project. 

### On select lecture dates we will have guest speakers

The topic we are studying is at the bleeding edge of applied practice in the data industry. As such it is useful to hear perspective on human centered design from leaders in industry. As their participation is confirmed it will be updated on the course schedule. 

## `r advdate(mon, 1)`

### Thurs Jan 21 {.heatinline}
#### Topic: Rstudio Global Conference 

 Register and attend any session(s) of the [Rstudio global conference](https://rstudio.com/conference/) 
  <br> 
  <br> 
  ![](https://rstudio.com/assets/img/rstudio-global-with-date.jpg) 
  

## `r advdate(mon, 2)`

### Tues Jan 26 {.heatinline}
#### Topic: Syllabus Day


### Thurs Jan 28 {.heatinline}
#### Topic: Introduction to Data Based Decisions 

`r emo::ji("book")` tfs (Introduction) <br>
nudge (Chapter 1)

```{r, echo=FALSE, results = "asis"}
emo::ji("scroll")
bib[c("bounthavong2019", "schneider2018")]
```


## `r advdate(mon, 3)`

**Project**: Start Problem Definition

### Tues Feb 2 {.heatinline}



#### Topic: Homo Economicus



```{r, echo = FALSE, results = "asis"}
emo::ji("scroll")
bib[c("f53")]
```

### Thurs Feb 4 {.heatinline}
#### Topic: R Markdown

```{r, echo=FALSE, results = "asis"}
emo::ji("scroll")
bib[c("hohman2020communicating", "Rmarkdown")]
```


## `r advdate(mon, 4)`

**Project**: Continue Problem Definition 



### Thursday Feb 11 {.heatinline}

No Class

## `r advdate(mon, 5)`

**Project**: Start User Persona

## `r advdate(mon, 6)`

**Project**: Continue User Persona



## `r advdate(mon, 7)`

**Project**: Start User Interviews


## `r advdate(mon, 8)`

**Project**: Continue User Interviews



## `r advdate(mon, 9)`

**Project**: Start Data Collection

## `r advdate(mon, 10)`

**Project**: Continue Data Collection


## `r advdate(mon, 11)`

**Project**: Build Prototype

### Tues March 30 {.heatinline}

No class



## `r advdate(mon, 12)`

**Project**: Continue Build Prototype

## `r advdate(mon, 13)`

**Project**: Collect User Feedback

## `r advdate(mon, 14)`

**Project**: Continue Collect User Feedback

## `r advdate(mon, 15)`
**Project**: Iterate and Finalize

## `r advdate(mon, 16)`
**Project**: Continue Iterate and Finalize



